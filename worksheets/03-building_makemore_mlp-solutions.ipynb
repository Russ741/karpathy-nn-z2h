{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a28808f6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "# Worksheet 3 - Multi-Layer Perceptron\n",
    "\n",
    "This is the fourth in a series of companion worksheets for for Andrej Karpathy's [Neural Networks: Zero To Hero](https://karpathy.ai/zero-to-hero.html) videos.\n",
    "\n",
    "It corresponds to the third video in the series, named \"[Building makemore Part 2: MLP](https://www.youtube.com/watch?v=TCH_1BHY58I)\".\n",
    "\n",
    "The rest of the worksheets are listed in the README [here](https://github.com/Russ741/karpathy-nn-z2h/).\n",
    "\n",
    "The overall objective of this worksheet is to write code that generates a word that is similar to a set of example words it is trained on.\n",
    "It does so using a multi-layer neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a147f0b4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Preamble: Load data\n",
    "\n",
    "Write a function that:\n",
    "* Loads the remotely-hosted [names.txt](https://github.com/karpathy/makemore/blob/master/names.txt) file\n",
    "([raw link](https://github.com/karpathy/makemore/raw/master/names.txt))\n",
    "\n",
    "And returns:\n",
    "* a list of strings (```words```)\n",
    "  * Each string should be equal to the word from the corresponding line of names.txt\n",
    "  * The strings should not include line-break characters\n",
    "\n",
    "Notes:\n",
    "* You can reuse your work from the previous worksheet for this if you like.\n",
    "* The test_words block below will save the loaded words as loaded_words for you to reuse later\n",
    "\n",
    "Video: [0:09:10](https://youtu.be/TCH_1BHY58I?t=550)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e38ec4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# The sample solution uses this library; if your code doesn't, feel free to remove it.\n",
    "import requests\n",
    "\n",
    "def load_words():\n",
    "# Solution code\n",
    "    words_url = 'https://raw.githubusercontent.com/karpathy/makemore/master/names.txt'\n",
    "    words = requests.get(words_url).text.splitlines()\n",
    "    return words\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e10d26",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_words():\n",
    "    if not isinstance(loaded_words, list):\n",
    "        print(f\"Expected words to be a list\")\n",
    "        return\n",
    "    if (len_words := len(loaded_words)) != (expected_words := 32033):\n",
    "        print(f\"Expected {expected_words} elements in words, found {len_words} elements\")\n",
    "        return\n",
    "    sorted_words = sorted(loaded_words)\n",
    "    if (zeroth_word := sorted_words[0]) != (expected_zeroth := \"aaban\"):\n",
    "        print(f\"Expected zeroth word in words to be '{expected_zeroth}', was '{zeroth_word}'\")\n",
    "        return\n",
    "    if (final_word := sorted_words[-1]) != (expected_final := \"zzyzx\"):\n",
    "        print(f\"Expected final word in words to be '{expected_final}', was '{final_word}'\")\n",
    "        return\n",
    "    print(\"load_words looks good. Onwards!\")\n",
    "loaded_words = load_words()\n",
    "test_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c78c02b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 1: Map characters to indices\n",
    "\n",
    "Write a function that takes the following arguments:\n",
    "* ```words``` (list of strings)\n",
    "\n",
    "And returns:\n",
    "* a dict (```stoi```) where\n",
    "  * the key is a character from ```words``` (including '.' for start/end),\n",
    "  * the value is a unique integer, and\n",
    "  * all the values are in the range from 0 to ```len(stoi) - 1``` (no gaps)\n",
    "\n",
    "We'll use these unique integers as an index to represent the characters in a Tensor in later steps\n",
    "\n",
    "Note that for this list of words, the same value of ```stoi``` could be generated without looking at the words at all,\n",
    "but simply by using all the lowercase letters and a period. This approach would be more efficient for this exercise,\n",
    "but will not generalize well conceptually to more complex models in future exercises.\n",
    "\n",
    "Video: [0:09:22](https://youtu.be/TCH_1BHY58I?t=562)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77907373",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_stoi(words):\n",
    "# Solution code\n",
    "    chars = set()\n",
    "    for word in words:\n",
    "        for char in word:\n",
    "            chars.add(char)\n",
    "    chars.add('.')\n",
    "    stoi = { v:k for (k, v) in enumerate(sorted(chars))}\n",
    "    return stoi\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee80e89",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def test_get_stoi():\n",
    "    bigrams = [\n",
    "        ('.', 'h'),\n",
    "        ('h', 'i'),\n",
    "        ('i', '.'),\n",
    "        ('.', 'b'),\n",
    "        ('b', 'y'),\n",
    "        ('y', 'e'),\n",
    "        ('e', '.'),\n",
    "    ]\n",
    "    expected_s = sorted(['.', 'h', 'i', 'b', 'y', 'e'])\n",
    "    stoi = get_stoi(bigrams)\n",
    "    if not isinstance(stoi, dict):\n",
    "        print(f\"Expected stoi to be a dict\")\n",
    "        return\n",
    "    s = sorted(stoi.keys())\n",
    "    if s != expected_s:\n",
    "        print(f\"Expected stoi keys to be {expected_s} when sorted, were {s}\")\n",
    "        return\n",
    "    expected_i = list(range(len(s)))\n",
    "    i = sorted(stoi.values())\n",
    "    if i != expected_i:\n",
    "        print(f\"Expected stoi values to be {expected_i} when sorted, were {i}\")\n",
    "        return\n",
    "    print(\"get_stoi looks good. Onwards!\")\n",
    "test_get_stoi()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2d1abb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 2: Map indices to characters\n",
    "\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* a dict (```stoi```) as defined in step 2\n",
    "\n",
    "And returns:\n",
    "* a dict (```itos```) where ```itos``` contains the same key-value pairs as ```stoi``` but with keys and values swapped.\n",
    "\n",
    "E.g. if ```stoi == {'.' : 0, 'b' : 1, 'z', 2}```, then ```itos == {0 : '.', 1 : 'b', 2 : 'z'}```\n",
    "\n",
    "Video: [0:09:22](https://youtu.be/TCH_1BHY58I?t=562)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c09999e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_itos(stoi):\n",
    "# Solution code\n",
    "    itos = {stoi[c]:c for c in stoi}\n",
    "    return itos\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cbc27d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def test_get_itos():\n",
    "    stoi = {elem:idx for idx, elem in enumerate(string.ascii_lowercase + \".\")}\n",
    "    itos = get_itos(stoi)\n",
    "    if not isinstance(itos, dict):\n",
    "        print(f\"Expected stoi to be a dict\")\n",
    "        return\n",
    "    for c in string.ascii_lowercase + \".\":\n",
    "        c_i = stoi[c]\n",
    "        if (expected_c := itos[c_i]) != c:\n",
    "            print(f\"Expected itos[{c_i}] to be {expected_c}, was {c}\")\n",
    "    print(\"get_itos looks good. Onwards!\")\n",
    "test_get_itos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe47633",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 3: Generate inputs ```X``` and outputs ```Y```\n",
    "\n",
    "Write a function that takes the following arguments:\n",
    "* a list of strings (```words``` from the preamble)\n",
    "* a dict of characters to integers (```stoi``` from step 2)\n",
    "* an integer (```block_size```) that specifies how many characters to take into account when predicting the next one\n",
    "\n",
    "And returns:\n",
    "* a two-dimensional torch.Tensor ```X``` with each sequence of characters of length block_size from the words in ```words```\n",
    "* a one-dimensional torch.Tensor ```Y``` with the character that follows each sequence in ```x```\n",
    "\n",
    "Video: [0:09:35](https://youtu.be/TCH_1BHY58I?t=575)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfd4bc7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_X_and_Y(words, stoi, block_size):\n",
    "    X = []\n",
    "    Y = []\n",
    "# Solution code\n",
    "    for word in words:\n",
    "        word = '.' * block_size + word + '.' * block_size\n",
    "        for idx in range(len(word) - block_size):\n",
    "            end = idx + block_size\n",
    "            chars = word[idx : end]\n",
    "            X.append([stoi[i] for i in chars])\n",
    "            Y.append(stoi[word[end]])\n",
    "    return torch.tensor(X), torch.tensor(Y)\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cdef88",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_get_X_and_Y():\n",
    "    words = [\n",
    "        \"hi\",\n",
    "        \"bye\",\n",
    "    ]\n",
    "    stoi = {\n",
    "        '.': 0,\n",
    "        'h': 1,\n",
    "        'i': 2,\n",
    "        'b': 3,\n",
    "        'y': 4,\n",
    "        'e': 5,\n",
    "    }\n",
    "    block_size = 3\n",
    "\n",
    "    (X, Y) = get_X_and_Y(words, stoi, block_size)\n",
    "\n",
    "    if not torch.is_tensor(X):\n",
    "        print(f\"Expected X to be a tensor, was {type(X)}\")\n",
    "        return\n",
    "    if not torch.is_tensor(Y):\n",
    "        print(f\"Expected Y to be a tensor, was {type(Y)}\")\n",
    "        return\n",
    "    expected_X = torch.tensor([\n",
    "        [0, 0, 0],\n",
    "        [0, 0, 1],\n",
    "        [0, 1, 2],\n",
    "        [1, 2, 0],\n",
    "        [2, 0, 0],\n",
    "        [0, 0, 0],\n",
    "        [0, 0, 3],\n",
    "        [0, 3, 4],\n",
    "        [3, 4, 5],\n",
    "        [4, 5, 0],\n",
    "        [5, 0, 0],\n",
    "    ])\n",
    "    expected_Y = torch.tensor([\n",
    "        1,\n",
    "        2,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        3,\n",
    "        4,\n",
    "        5,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "    ])\n",
    "    if (shape_X := X.shape) != (expected_shape_X := expected_X.shape):\n",
    "        print(f\"Expected shape of X for test case to be {expected_shape_X}, was {shape_X}\")\n",
    "        return\n",
    "    if not X.equal(expected_X):\n",
    "        print(f\"Expected X for test case to be {expected_X}, was {X}\")\n",
    "        return\n",
    "    if not Y.equal(expected_Y):\n",
    "        print(f\"Expected Y for test case to be {expected_Y}, was {Y}\")\n",
    "        return\n",
    "    print(\"get_x_and_y looks good. Onwards!\")\n",
    "test_get_X_and_Y()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a524be",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 4: Initialize vector embedding lookup table ```C```\n",
    "\n",
    "Write a function that takes the following arguments:\n",
    "* An integer (```indices```) representing the number of indices in ```stoi``` to embed\n",
    "* An integer (```embed_dimensions```) representing the number of dimensions the embedded vectors will have\n",
    "* A ```torch.Generator``` (```gen```) to provide (pseudo)random initial values for the parameters\n",
    "\n",
    "And returns:\n",
    "* a ```torch.Tensor``` of ```float64``` (```C```) representing the random initial vector for each index.\n",
    "\n",
    "Video: [0:12:19](https://youtu.be/TCH_1BHY58I?t=739)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af8fab8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_C(indices, embed_dimensions, gen):\n",
    "# Solution code\n",
    "    return torch.rand((indices, embed_dimensions), dtype=torch.float64, generator=gen)\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd78b62",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_get_C():\n",
    "    indices = 7\n",
    "    embed_dimensions = 4\n",
    "    gen = torch.Generator()\n",
    "    gen.manual_seed(12345)\n",
    "    C = get_C(indices, embed_dimensions, gen)\n",
    "    if not torch.is_tensor(C):\n",
    "        print(f\"Expected C to be a tensor, was {type(C)}\")\n",
    "        return\n",
    "    if not torch.is_floating_point(C):\n",
    "        print(f\"Expected C to be a tensor of floating point.\")\n",
    "        return\n",
    "    if (shape_C := C.shape) != (expected_shape_C := (indices, embed_dimensions)):\n",
    "        print(f\"Expected shape of C for test case to be {expected_shape_C}, was {shape_C}\")\n",
    "        return\n",
    "    for i in range(len(C)):\n",
    "        for j in range(len(C)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            if C[i].equal(C[j]):\n",
    "                print(f\"Rows {i} and {j} of C are too similar.\")\n",
    "                print(f\"{C[i]=}\")\n",
    "                print(f\"{C[j]=}\")\n",
    "                return\n",
    "    print(\"get_C looks good. Onwards!\")\n",
    "test_get_C()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31f212d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 5: Generate vector embeddings of X\n",
    "\n",
    "Write a function that takes the following arguments:\n",
    "* a two-dimensional torch.Tensor ```X``` as defined in step 3\n",
    "* a two-dimensional torch.Tensor ```C``` as defined in step 4\n",
    "\n",
    "And returns:\n",
    "* a **two**-dimensional torch.Tensor ```emb``` where each row is the concatenated vector embeddings of the indices of the corresponding row in X\n",
    "  * Note the slight difference from the video, where emb is *three*-dimensional\n",
    "\n",
    "Note that the vector embeddings in a row in C theoretically do not need to match the order of the indices in the row in X;\n",
    "they only need to be consistent with the other rows in C.\n",
    "For this worksheet, though, if the order does differ, the test case will fail.\n",
    "\n",
    "Video: [0:13:07](https://youtu.be/TCH_1BHY58I?t=787) and [0:19:10](https://youtu.be/TCH_1BHY58I?t=1150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adf9a31",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_emb(X, C):\n",
    "# Solution code\n",
    "    return C[X].reshape(len(X), -1)\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2c6b40",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_get_vector_embedding():\n",
    "    X = torch.tensor([\n",
    "        [1, 2],\n",
    "        [2, 1],\n",
    "        [0, 1],\n",
    "    ])\n",
    "    ZERO = [0.1, 0.2, 0.3]\n",
    "    ONE = [0.4, 0.5, 0.6]\n",
    "    TWO = [0.7, 0.8, 0.9]\n",
    "    C = torch.tensor([\n",
    "        ZERO,\n",
    "        ONE,\n",
    "        TWO,\n",
    "    ])\n",
    "\n",
    "    emb = get_emb(X, C)\n",
    "\n",
    "    expected_emb = torch.tensor([\n",
    "        ONE + TWO,\n",
    "        TWO + ONE,\n",
    "        ZERO + ONE,\n",
    "    ])\n",
    "    if not emb.equal(expected_emb):\n",
    "        print(f\"Expected emb to be \\n{expected_emb}\\n, was \\n{emb}\")\n",
    "        return\n",
    "    print(\"get_vector_embedding looks good. Onwards!\")\n",
    "test_get_vector_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4c1cb8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 6: Initialize hidden layer coefficients\n",
    "\n",
    "Write a function that takes the following arguments:\n",
    "* the number of inputs (```input_ct```) to each neuron in the hidden layer\n",
    "  * Equal to the number of cells in each row of emb\n",
    "  * In the video, this is given the value 6\n",
    "* the number of neurons (```neuron_ct```) to include in the hidden layer\n",
    "  * In the video, this is given the value 100\n",
    "\n",
    "And returns:\n",
    "* a two-dimensional ```torch.Tensor``` ```W``` of shape (```input_ct```, ```neuron_ct```) of type ```torch.float64```\n",
    "  * each element of ```W``` should be randomly generated\n",
    "* a one-dimensional pytorch.Tensor ```b``` of length ```neuron_ct```\n",
    "  * the elements of ```b``` can be zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdc8a9f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def initialize_W_b(input_ct, neuron_ct):\n",
    "# Solution code\n",
    "    W = torch.rand((input_ct, neuron_ct), dtype=torch.float64, requires_grad=True)\n",
    "    b = torch.zeros(neuron_ct, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "    return W, b\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a832bbdd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_initialize_W_b():\n",
    "    input_ct = 3\n",
    "    neuron_ct = 5\n",
    "    W, b = initialize_W_b(input_ct, neuron_ct)\n",
    "    if not torch.is_tensor(W):\n",
    "        print(\"Expected W to be a tensor\")\n",
    "        return\n",
    "    if not torch.is_tensor(b):\n",
    "        print(\"Expected B to be a tensor\")\n",
    "        return\n",
    "    if not W.is_floating_point():\n",
    "        print(\"Expected W to be a tensor of floating point numbers\")\n",
    "        return\n",
    "    if not b.is_floating_point():\n",
    "        print(\"Expected b to be a tensor of floating point numbers\")\n",
    "        return\n",
    "    if (W_shape := W.shape) != (expected_W_shape := (input_ct, neuron_ct)):\n",
    "        print(f\"Expected W shape to be {expected_W_shape}, was {W_shape}\")\n",
    "        return\n",
    "    # The comma is required to make expected_b_shape into a single-element tuple\n",
    "    if (b_shape := b.shape) != (expected_b_shape := (neuron_ct,)):\n",
    "        print(f\"Expected b shape to be {expected_b_shape}, was {b_shape}\")\n",
    "        return\n",
    "    print(\"W and b look good. Onwards!\")\n",
    "test_initialize_W_b()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45b0cc8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 7: Forward propagate through hidden layer\n",
    "\n",
    "Write a function that takes the following arguments:\n",
    "* a two-dimensional ```torch.Tensor``` ```emb``` as defined in step 5\n",
    "  * This is the input to the hidden layer\n",
    "* a two-dimensional ```torch.Tensor``` ```W``` as defined in step 6\n",
    "  * This is the hidden layer's weights\n",
    "* a one-dimensional ```torch.Tensor``` ```b``` as defined in step 6\n",
    "  * This is the hidden layer's biases\n",
    "\n",
    "And returns:\n",
    "* a one-dimensional ```torch.Tensor``` ```h```\n",
    "  * This is the output of the hidden layer after applying a tanh activation function\n",
    "\n",
    "Video: [0:19:14](https://youtu.be/TCH_1BHY58I?t=1155) and [0:27:57](https://youtu.be/TCH_1BHY58I?t=1677)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7775c182",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_h(emb, W, b):\n",
    "# Solution code\n",
    "    return torch.tanh(emb @ W + b)\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb19518",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_get_h():\n",
    "    emb = torch.tensor([\n",
    "        [0.1, 0.2],\n",
    "        [-.3, 0.4],\n",
    "        [.05, -.06],\n",
    "    ], dtype=torch.float64)\n",
    "    W = torch.tensor([\n",
    "        [0.7, 0.8, -0.9, -0.1],\n",
    "        [0.6, 0.5, 0.4, 0.3],\n",
    "    ], dtype=torch.float64)\n",
    "    b = torch.tensor([\n",
    "        .09, -.01, .011, -.012\n",
    "    ], dtype=torch.float64)\n",
    "    h = get_h(emb, W, b)\n",
    "    expected_h = torch.tensor([\n",
    "        [ 2.7291e-01,  1.6838e-01,  1.0000e-03,  3.7982e-02],\n",
    "        [ 1.1943e-01, -4.9958e-02,  4.1447e-01,  1.3713e-01],\n",
    "        [ 8.8766e-02,  8.6736e-18, -5.7935e-02, -3.4986e-02],\n",
    "    ], dtype=torch.float64)\n",
    "    if not torch.isclose(expected_h, h, rtol = 0.0, atol = 0.0001).all():\n",
    "        print(f\"Expected h for test case to be \\n{expected_h}\\n, was \\n{h}\")\n",
    "        return\n",
    "    print(\"get_h looks good. Onwards!\")\n",
    "test_get_h()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13593d3c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step : Initialize output layer coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c84f8af",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step : Forward propagate through output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623e2275",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step : Calculate loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffde22d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step : Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4271ccdc",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step : Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398020a5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 3
   },
   "source": [
    "### Step : Generate words"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
