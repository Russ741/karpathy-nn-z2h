{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf99c375",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "\n",
    "# Worksheet 3 - Multi-Layer Perceptron\n",
    "\n",
    "This is the fourth in a series of companion worksheets for for Andrej Karpathy's [Neural Networks: Zero To Hero](https://karpathy.ai/zero-to-hero.html) videos.\n",
    "\n",
    "It corresponds to the third video in the series, named \"[Building makemore Part 2: MLP](https://www.youtube.com/watch?v=TCH_1BHY58I)\".\n",
    "\n",
    "The rest of the worksheets are listed in the README [here](https://github.com/Russ741/karpathy-nn-z2h/).\n",
    "\n",
    "The overall objective of this worksheet is to write code that generates a word that is similar to a set of example words it is trained on.\n",
    "It does so using a multi-layer neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5e36b3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Prerequisite: Load worksheet utilities and download word list\n",
    "\n",
    "The following cell imports [utility functions](https://github.com/Russ741/karpathy-nn-z2h/blob/main/worksheets/worksheet_utils.py) that this worksheet depends on.\n",
    "If the file isn't already locally available (e.g. for Colab), it downloads it from GitHub.\n",
    "\n",
    "Similarly, if this directory does not already contain names.txt, it downloads it from\n",
    "[the makemore GitHub repository](https://github.com/karpathy/makemore/blob/master/names.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41aaddb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import shutil\n",
    "\n",
    "try:\n",
    "    from worksheet_utils import *\n",
    "    print(\"worksheet_utils found.\")\n",
    "except ModuleNotFoundError:\n",
    "    utils_local_filename = \"worksheet_utils.py\"\n",
    "    print(f\"Downloading worksheet_utils.\")\n",
    "    with urllib.request.urlopen(\"https://raw.githubusercontent.com/Russ741/karpathy-nn-z2h/main/worksheets/worksheet_utils.py\") as response:\n",
    "        with open(utils_local_filename, mode=\"xb\") as utils_file:\n",
    "            shutil.copyfileobj(response, utils_file)\n",
    "    from worksheet_utils import *\n",
    "\n",
    "WORDS_PATH = \"names.txt\"\n",
    "if os.path.isfile(WORDS_PATH):\n",
    "    print(\"word file found.\")\n",
    "else:\n",
    "    print(\"word file not found, downloading.\")\n",
    "    with urllib.request.urlopen(\"https://github.com/karpathy/makemore/raw/master/names.txt\") as response:\n",
    "        with open(WORDS_PATH, mode=\"xb\") as words_file:\n",
    "            shutil.copyfileobj(response, words_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bf9a07",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Preamble: Load data\n",
    "\n",
    "Objective: Write a function that:\n",
    " * Returns a list of strings\n",
    "   * Each string should be equal to the word from the corresponding line of the word file (at ```WORDS_PATH```)\n",
    "   * The strings should not include line-break characters\n",
    "\n",
    "Note: In practice, the order of the strings in the returned list does not matter, but for the\n",
    "test to pass, they should be in the same order in the list as in the word file.\n",
    "\n",
    "Video: [0:09:10](https://youtu.be/TCH_1BHY58I?t=550)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfb27ce",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_words():\n",
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e455302b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_words():\n",
    "    expect_type(\"loaded_words\", loaded_words, list)\n",
    "    expect_eq(\"len(loaded_words)\", len(loaded_words), 32033)\n",
    "    expect_eq(\"loaded_words[0]\", loaded_words[0], \"emma\")\n",
    "    expect_eq(\"loaded_words[-1]\", loaded_words[-1], \"zzyzx\")\n",
    "    print(\"load_words looks good. Onwards!\")\n",
    "loaded_words = load_words()\n",
    "test_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ed4466",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 1: Map characters to indices\n",
    "\n",
    "Write a function that takes the following arguments:\n",
    "* ```words``` (list of strings)\n",
    "\n",
    "And returns:\n",
    "* a dict (```stoi```) where\n",
    "  * the key is a character from ```words``` (including '.' for start/end),\n",
    "  * the value is a unique integer, and\n",
    "  * all the values are in the range from 0 to ```len(stoi) - 1``` (no gaps)\n",
    "\n",
    "We'll use these unique integers as an index to represent the characters in a Tensor in later steps\n",
    "\n",
    "Note that for this list of words, the same value of ```stoi``` could be generated without looking at the words at all,\n",
    "but simply by using all the lowercase letters and a period. This approach would be more efficient for this exercise,\n",
    "but will not generalize well conceptually to more complex models in future exercises.\n",
    "\n",
    "Video: [0:09:22](https://youtu.be/TCH_1BHY58I?t=562)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ef782d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_stoi(words):\n",
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e24291",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def test_get_stoi():\n",
    "    bigrams = [\n",
    "        ('.', 'h'),\n",
    "        ('h', 'i'),\n",
    "        ('i', '.'),\n",
    "        ('.', 'b'),\n",
    "        ('b', 'y'),\n",
    "        ('y', 'e'),\n",
    "        ('e', '.'),\n",
    "    ]\n",
    "\n",
    "    stoi = get_stoi(bigrams)\n",
    "\n",
    "    expect_type(\"stoi\", stoi, dict)\n",
    "    s = sorted(stoi.keys())\n",
    "    expected_s = sorted(['.', 'h', 'i', 'b', 'y', 'e'])\n",
    "    expect_eq(\"stoi keys when sorted\", s, expected_s)\n",
    "    i = sorted(stoi.values())\n",
    "    expected_i = list(range(len(s)))\n",
    "    expect_eq(\"stoi values when sorted\", i, expected_i)\n",
    "    print(\"get_stoi looks good. Onwards!\")\n",
    "test_get_stoi()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6603e1b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 2: Map indices to characters\n",
    "\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* a dict (```stoi```) as defined in step 2\n",
    "\n",
    "And returns:\n",
    "* a dict (```itos```) where ```itos``` contains the same key-value pairs as ```stoi``` but with keys and values swapped.\n",
    "\n",
    "E.g. if ```stoi == {'.' : 0, 'b' : 1, 'z', 2}```, then ```itos == {0 : '.', 1 : 'b', 2 : 'z'}```\n",
    "\n",
    "Video: [0:09:22](https://youtu.be/TCH_1BHY58I?t=562)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f179c5e6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_itos(stoi):\n",
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c0410c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def test_get_itos():\n",
    "    stoi = {elem:idx for idx, elem in enumerate(string.ascii_lowercase + \".\")}\n",
    "    itos = get_itos(stoi)\n",
    "    expect_type(\"itos\", itos, dict)\n",
    "    for c in string.ascii_lowercase + \".\":\n",
    "        c_i = stoi[c]\n",
    "        expect_eq(f\"itos[{c_i}]\", itos[c_i], c)\n",
    "    print(\"get_itos looks good. Onwards!\")\n",
    "test_get_itos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c984609",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 3: Generate inputs ```X``` and outputs ```Y```\n",
    "\n",
    "Write a function that takes the following arguments:\n",
    "* a list of strings (```words``` from the preamble)\n",
    "* a dict of characters to integers (```stoi``` from step 2)\n",
    "* an integer (```block_size```) that specifies how many characters to take into account when predicting the next one\n",
    "\n",
    "And returns:\n",
    "* a two-dimensional torch.Tensor ```X``` with each sequence of characters of length block_size from the words in ```words```\n",
    "* a one-dimensional torch.Tensor ```Y``` with the character that follows each sequence in ```x```\n",
    "\n",
    "Video: [0:09:35](https://youtu.be/TCH_1BHY58I?t=575)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdda3a69",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_X_and_Y(words, stoi, block_size):\n",
    "    X = []\n",
    "    Y = []\n",
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91f3ba1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_get_X_and_Y():\n",
    "    words = [\n",
    "        \"hi\",\n",
    "        \"bye\",\n",
    "    ]\n",
    "    stoi = {\n",
    "        '.': 0,\n",
    "        'h': 1,\n",
    "        'i': 2,\n",
    "        'b': 3,\n",
    "        'y': 4,\n",
    "        'e': 5,\n",
    "    }\n",
    "    block_size = 3\n",
    "\n",
    "    (X, Y) = get_X_and_Y(words, stoi, block_size)\n",
    "\n",
    "    expected_X = torch.tensor([\n",
    "        [0, 0, 0],\n",
    "        [0, 0, 1],\n",
    "        [0, 1, 2],\n",
    "        [0, 0, 0],\n",
    "        [0, 0, 3],\n",
    "        [0, 3, 4],\n",
    "        [3, 4, 5],\n",
    "    ])\n",
    "    expected_Y = torch.tensor([\n",
    "        1,\n",
    "        2,\n",
    "        0,\n",
    "        3,\n",
    "        4,\n",
    "        5,\n",
    "        0,\n",
    "    ])\n",
    "    expect_tensor_close(\"X for test case\", X, expected_X)\n",
    "    expect_tensor_close(\"Y for test case\", Y, expected_Y)\n",
    "    print(\"get_x_and_y looks good. Onwards!\")\n",
    "test_get_X_and_Y()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05b0c03",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 4: Initialize vector embedding lookup table ```C```\n",
    "\n",
    "Write a function that takes the following arguments:\n",
    "* An integer (```indices```) representing the number of indices in ```stoi``` to embed\n",
    "* An integer (```embed_dimensions```) representing the number of dimensions the embedded vectors will have\n",
    "* A ```torch.Generator``` (```gen```) to provide (pseudo)random initial values for the parameters\n",
    "\n",
    "And returns:\n",
    "* a ```torch.Tensor``` of ```float64``` (```C```) representing the random initial vector for each index.\n",
    "\n",
    "Video: [0:12:19](https://youtu.be/TCH_1BHY58I?t=739)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89239d3f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_C(indices, embed_dimensions, gen):\n",
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46893743",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_get_C():\n",
    "    indices = 7\n",
    "    embed_dimensions = 4\n",
    "    gen = torch.Generator()\n",
    "    gen.manual_seed(12345)\n",
    "    C = get_C(indices, embed_dimensions, gen)\n",
    "    expect_type(\"C\", C, torch.Tensor)\n",
    "    expect_eq(\"C.dtype\", C.dtype, torch.float64)\n",
    "    expect_eq(\"C.shape\", C.shape, (indices, embed_dimensions))\n",
    "    for i in range(len(C)):\n",
    "        for j in range(len(C)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            if C[i].equal(C[j]):\n",
    "                raise Exception(f\"Rows {i} and {j} of C are too similar.\\n{C[i]=}\\n{C[j]=}\")\n",
    "    print(\"get_C looks good. Onwards!\")\n",
    "test_get_C()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d90e4c9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 5: Generate vector embeddings of X\n",
    "\n",
    "Write a function that takes the following arguments:\n",
    "* a two-dimensional torch.Tensor ```X``` as defined in step 3\n",
    "* a two-dimensional torch.Tensor ```C``` as defined in step 4\n",
    "\n",
    "And returns:\n",
    "* a **two**-dimensional torch.Tensor ```emb``` where each row is the concatenated vector embeddings of the indices of the corresponding row in X\n",
    "  * Note the slight difference from the video, where emb is *three*-dimensional\n",
    "\n",
    "Note that the vector embeddings in a row in C theoretically do not need to match the order of the indices in the row in X;\n",
    "they only need to be consistent with the other rows in C.\n",
    "For this worksheet, though, if the order does differ, the test case will fail.\n",
    "\n",
    "Video: [0:13:07](https://youtu.be/TCH_1BHY58I?t=787) and [0:19:10](https://youtu.be/TCH_1BHY58I?t=1150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cbf839",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_emb(X, C):\n",
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a13f380",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_get_vector_embedding():\n",
    "    X = torch.tensor([\n",
    "        [1, 2],\n",
    "        [2, 1],\n",
    "        [0, 1],\n",
    "    ])\n",
    "    ZERO = [0.1, 0.2, 0.3]\n",
    "    ONE = [0.4, 0.5, 0.6]\n",
    "    TWO = [0.7, 0.8, 0.9]\n",
    "    C = torch.tensor([\n",
    "        ZERO,\n",
    "        ONE,\n",
    "        TWO,\n",
    "    ])\n",
    "\n",
    "    emb = get_emb(X, C)\n",
    "\n",
    "    expected_emb = torch.tensor([\n",
    "        ONE + TWO,\n",
    "        TWO + ONE,\n",
    "        ZERO + ONE,\n",
    "    ])\n",
    "    expect_tensor_close(\"emb\", emb, expected_emb)\n",
    "    print(\"get_vector_embedding looks good. Onwards!\")\n",
    "test_get_vector_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d6ba11",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 6: Initialize weight and bias coefficients\n",
    "\n",
    "Write a function that takes the following arguments:\n",
    "* the number of inputs (```input_ct```) to each neuron in the current layer\n",
    "  * For the hidden layer, this is equal to the number of cells in each row of emb\n",
    "  * For the output layer, this is equal to the number of neurons in the previous (hidden) layer\n",
    "* the number of neurons (```neuron_ct```) to include in the current layer\n",
    "  * Karpathy chooses to have 100 neurons for the hidden layer\n",
    "  * The output layer should have one neuron for each possible result\n",
    "* A ```torch.Generator``` (```gen```) to provide (pseudo)random initial values for the parameters\n",
    "\n",
    "And returns:\n",
    "* a two-dimensional ```torch.Tensor``` ```W``` of shape (```input_ct```, ```neuron_ct```) of type ```torch.float64```\n",
    "  * each element of ```W``` should be randomly generated\n",
    "* a one-dimensional pytorch.Tensor ```b``` of length ```neuron_ct```\n",
    "  * the elements of ```b``` can be zero\n",
    "\n",
    "Video: [0:29:17](https://youtu.be/TCH_1BHY58I?t=1757)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b50f91",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def initialize_W_b(input_ct, neuron_ct, gen):\n",
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ab82fd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_initialize_W_b():\n",
    "    input_ct = 3\n",
    "    neuron_ct = 5\n",
    "    gen = torch.Generator()\n",
    "    gen.manual_seed(12345)\n",
    "    W, b = initialize_W_b(input_ct, neuron_ct, gen)\n",
    "    expect_type(\"W\", W, torch.Tensor)\n",
    "    expect_type(\"b\", b, torch.Tensor)\n",
    "    expect_eq(\"W.dtype\", W.dtype, torch.float64)\n",
    "    expect_eq(\"b.dtype\", b.dtype, torch.float64)\n",
    "    expect_eq(\"W.shape\", W.shape, (input_ct, neuron_ct))\n",
    "    # The comma is required to make expected_b_shape into a single-element tuple\n",
    "    expect_eq(\"b.shape\", b.shape, (neuron_ct,))\n",
    "    print(\"W and b look good. Onwards!\")\n",
    "test_initialize_W_b()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051842fc",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 7: Forward propagate through hidden layer\n",
    "\n",
    "Write a function that takes the following arguments:\n",
    "* a two-dimensional ```torch.Tensor``` ```emb``` as defined in step 5\n",
    "  * This is the input to the hidden layer\n",
    "* a two-dimensional ```torch.Tensor``` ```W1``` as defined in step 6\n",
    "  * This is the hidden layer's weights\n",
    "* a one-dimensional ```torch.Tensor``` ```b1``` as defined in step 6\n",
    "  * This is the hidden layer's biases\n",
    "\n",
    "And returns:\n",
    "* a one-dimensional ```torch.Tensor``` ```h```\n",
    "  * This is the output of the hidden layer after applying a tanh activation function\n",
    "\n",
    "Video: [0:19:14](https://youtu.be/TCH_1BHY58I?t=1155) and [0:27:57](https://youtu.be/TCH_1BHY58I?t=1677)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a012816",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_h(emb, W1, b1):\n",
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3886bf8f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_get_h():\n",
    "    emb = torch.tensor([\n",
    "        [0.1, 0.2],\n",
    "        [-.3, 0.4],\n",
    "        [.05, -.06],\n",
    "    ], dtype=torch.float64)\n",
    "    W1 = torch.tensor([\n",
    "        [0.7, 0.8, -0.9, -0.1],\n",
    "        [0.6, 0.5, 0.4, 0.3],\n",
    "    ], dtype=torch.float64)\n",
    "    b1 = torch.tensor([\n",
    "        .09, -.01, .011, -.012\n",
    "    ], dtype=torch.float64)\n",
    "\n",
    "    h = get_h(emb, W1, b1)\n",
    "\n",
    "    expected_h = torch.tensor([\n",
    "        [ 2.7291e-01,  1.6838e-01,  1.0000e-03,  3.7982e-02],\n",
    "        [ 1.1943e-01, -4.9958e-02,  4.1447e-01,  1.3713e-01],\n",
    "        [ 8.8766e-02,  8.6736e-18, -5.7935e-02, -3.4986e-02],\n",
    "    ], dtype=torch.float64)\n",
    "    expect_tensor_close(\"h for test case\", h, expected_h)\n",
    "    print(\"get_h looks good. Onwards!\")\n",
    "test_get_h()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2b5db0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 8: Calculate output layer outputs before activation\n",
    "\n",
    "Write a function that takes the following arguments:\n",
    "* a two-dimensional ```torch.Tensor``` ```W2``` as defined in step 6\n",
    "  * This is the output layer's weights\n",
    "* a one-dimensional ```torch.Tensor``` ```b2``` as defined in step 6\n",
    "  * This is the output layer's biases\n",
    "\n",
    "And returns:\n",
    "* a one-dimensional ```torch.Tensor``` ```logits```\n",
    "  * This is the output of the output layer before applying an activation function\n",
    "\n",
    "Video: [0:29:15](https://youtu.be/TCH_1BHY58I?t=1755)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e98a5ad",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_logits(h, W2, b2):\n",
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cf3ae6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_get_logits():\n",
    "    h = torch.tensor([\n",
    "        [1.0, 2.0],\n",
    "        [3.0, 4.0],\n",
    "        [5.0, 6.0]\n",
    "    ])\n",
    "    W2 = torch.tensor([\n",
    "        [10.0, 10.1, 11.0, 19.9],\n",
    "        [100.0, -101.1, 0.0, 98.7],\n",
    "    ])\n",
    "    b2 = torch.tensor([\n",
    "        3.0, 5.0, 11.0, 13.0,\n",
    "    ])\n",
    "\n",
    "    logits = get_logits(h, W2, b2)\n",
    "\n",
    "    expected_logits = torch.tensor([\n",
    "        [ 213.0, -187.1,   22.0,  230.3],\n",
    "        [ 433.0, -369.1,   44.0,  467.5],\n",
    "        [ 653.0, -551.1,   66.0,  704.7],\n",
    "    ])\n",
    "    expect_tensor_close(\"logits\", logits, expected_logits)\n",
    "    print(\"get_logits looks good. Onward!\")\n",
    "test_get_logits()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9c686f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 9: Calculate output softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f36de6f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_prob(logits):\n",
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2287fa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def test_get_prob():\n",
    "    logits = torch.tensor([\n",
    "        [log(1), log(2), log(3), log(4)],\n",
    "        [ 0.123,  0.123,  0.123,  0.123],\n",
    "    ])\n",
    "\n",
    "    prob = get_prob(logits)\n",
    "\n",
    "    expected_prob = torch.tensor([\n",
    "        [ 0.1,  0.2,  0.3,  0.4],\n",
    "        [0.25, 0.25, 0.25, 0.25]\n",
    "    ])\n",
    "    expect_tensor_close(\"prob for test case\", prob, expected_prob)\n",
    "    print(\"get_prob looks good. Onward!\")\n",
    "test_get_prob()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a7b566",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 10: Forward propagate from vector embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dc21ab",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def forward_prop(emb, W1, b1, W2, b2):\n",
    "    h = get_h(emb, W1, b1)\n",
    "    logits = get_logits(h, W2, b2)\n",
    "    y_hat = get_prob(logits)\n",
    "\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fff304",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_forward_prop():\n",
    "    emb = torch.tensor([\n",
    "        [ 1.2,  2.1],\n",
    "        [-0.5, -0.7],\n",
    "        [ 0.0,  0.5247],\n",
    "        [-4.0, 3.1]\n",
    "    ])\n",
    "    W1 = torch.tensor([\n",
    "        [2.3, 0.9, 0.7],\n",
    "        [-3.2, 0.8, 1.3],\n",
    "    ])\n",
    "    b1 = torch.tensor([\n",
    "        0.2, 1.1, -0.1\n",
    "    ])\n",
    "    W2 = torch.tensor([\n",
    "        [ 3.4,  4.5],\n",
    "        [ 0.6,  0.5],\n",
    "        [-1.2,  2.2]\n",
    "    ])\n",
    "    b2 = torch.tensor([\n",
    "        0.3, -0.4,\n",
    "    ])\n",
    "\n",
    "    y_hat = forward_prop(emb, W1, b1, W2, b2)\n",
    "\n",
    "    expected_y_hat = torch.tensor([\n",
    "        [0.1832, 0.8168],\n",
    "        [0.9396, 0.0604],\n",
    "        [0.5000, 0.5000],\n",
    "        [0.2770, 0.7230],\n",
    "    ])\n",
    "    expect_tensor_close(\"y_hat\", y_hat, expected_y_hat)\n",
    "    print(\"forward_prop looks good. Onward!\")\n",
    "test_forward_prop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4bd3b0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 11: Loss calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a29ebba",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_loss(Y_hat, Y):\n",
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010a0684",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_get_loss():\n",
    "    Y_hat = torch.tensor([\n",
    "        [1.0, 0.0, 0.0, 0.0],\n",
    "        [0.0, 0.0, 0.0, 1.0],\n",
    "        [0.4512, 0.44933, 0.0, 0.0],\n",
    "        [0.05, 0.05, 0.81873, 0.08127],\n",
    "    ])\n",
    "    Y = torch.tensor([\n",
    "        0,\n",
    "        3,\n",
    "        1,\n",
    "        2,\n",
    "    ])\n",
    "    neg_log_likelihood = get_loss(Y_hat, Y)\n",
    "    expect_close(\"negative loss likelihood\", neg_log_likelihood, 0.25)\n",
    "    print(\"get_loss looks good. Onward!\")\n",
    "test_get_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf605b7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 12: Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deabd2b4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def descend_gradient(t, learning_rate):\n",
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7508da18",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_descend_gradient():\n",
    "    t = torch.tensor([\n",
    "        [1.0, 2.0, 3.0],\n",
    "        [4.0, 5.0, 6.0]\n",
    "    ])\n",
    "    t.grad = torch.tensor([\n",
    "        [0.5, 0.3, 0.1],\n",
    "        [-0.2, -0.4, -0.6]\n",
    "    ])\n",
    "    learning_rate = 2.0\n",
    "\n",
    "    descend_gradient(t, learning_rate)\n",
    "\n",
    "    expected_t = torch.tensor([\n",
    "        [0.0, 1.4, 2.8],\n",
    "        [4.4, 5.8, 7.2]\n",
    "    ])\n",
    "    expect_tensor_close(\"t\", t, expected_t)\n",
    "    print(\"descend_gradient looks good. Onward!\")\n",
    "test_descend_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1589a2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 13: Train model once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9a7f12",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_once(X, Y, C, W1, b1, W2, b2, learning_rate):\n",
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10157922",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_train_once():\n",
    "    pass\n",
    "test_train_once()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab87d9b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 14: Generate a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161e16f6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generate_word(C, block_size, W1, b1, W2, b2, stoi, itos, gen):\n",
    "    chr = '.'\n",
    "    word = \"\"\n",
    "    while True:\n",
    "        block = ('.' * block_size + word)[-3:]\n",
    "        idxes = [stoi[c] for c in block]\n",
    "        x = torch.tensor([idxes])\n",
    "        emb = get_emb(x, C)\n",
    "        probability_distribution = forward_prop(emb, W1, b1, W2, b2)\n",
    "        sample = torch.multinomial(probability_distribution, 1, generator=gen).item()\n",
    "        chr = itos[sample]\n",
    "        if chr == '.':\n",
    "            break\n",
    "        word += chr\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b60780",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_generate_word():\n",
    "    pass\n",
    "test_generate_word()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e16fb74",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 15: Train the model repeatedly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b20b976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2738b2ae",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 15: Generate words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6492c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19b9ca1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Bonus: Calculate probability of an empty word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f0119e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_empty_word_prob(C, W1, b1, W2, b2, stoi):\n",
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2220db",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "prob_empty = get_empty_word_prob(C, W1, b1, W2, b2, stoi)\n",
    "print(f\"The probability of this model generating an empty word is {prob_empty}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba07cced",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
