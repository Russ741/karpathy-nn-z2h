{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eb016f2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "\n",
    "# Worksheet 2a - Probability Table\n",
    "\n",
    "This is the second in a series of companion worksheets for for Andrej Karpathy's [Neural Networks: Zero To Hero](https://karpathy.ai/zero-to-hero.html) videos.\n",
    "\n",
    "It corresponds to roughly the first half of the second video in the series, named \"[The spelled-out intro to language modeling: building makemore](https://www.youtube.com/watch?v=PaCmpygFfXo)\".\n",
    "\n",
    "The first worksheet in the series is provided by Andrej, and can be found [here](https://colab.research.google.com/drive/1FPTx1RXtBfc4MaTkf7viZZD4U2F9gtKN?usp=sharing).\n",
    "\n",
    "The rest of the worksheets are listed in the README [here](https://github.com/Russ741/karpathy-nn-z2h/).\n",
    "\n",
    "The overall objective of this worksheet is to write code that generates a word that is similar to a set of example words it is trained on.\n",
    "\n",
    "Note that this worksheet uses a probability table, *not* neural networks like subsequent neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff27298",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Preamble: Load data\n",
    "\n",
    "Objective: Load a list of words from the remotely-hosted [names.txt](https://github.com/karpathy/makemore/blob/master/names.txt) file\n",
    "([raw link](https://github.com/karpathy/makemore/raw/master/names.txt)) into a list variable named ```words```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7342847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution code\n",
    "\n",
    "# To load names.txt from the makemore GitHub page:\n",
    "import requests\n",
    "\n",
    "words_url = 'https://raw.githubusercontent.com/karpathy/makemore/master/names.txt'\n",
    "words = requests.get(words_url).text.splitlines()\n",
    "\n",
    "# To load names.txt from a local file after downloading it:\n",
    "# # curl https://raw.githubusercontent.com/karpathy/makemore/master/names.txt > names.txt\n",
    "#\n",
    "# # read() gets the file as one long string with line breaks in it\n",
    "# # splitlines() divides the whole-file string into a list of strings and removes the line breaks\n",
    "# words = open(\"names.txt\").read().splitlines()\n",
    "\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36eb71c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_words():\n",
    "    if not isinstance(words, list):\n",
    "        print(f\"Expected words to be a list\")\n",
    "        return\n",
    "    if (len_words := len(words)) != (expected_words := 32033):\n",
    "        print(f\"Expected {expected_words} elements in words, found {len_words} elements\")\n",
    "        return\n",
    "    if (zeroth_word := words[0]) != (expected_zeroth := \"emma\"):\n",
    "        print(f\"Expected zeroth word in words to be '{expected_zeroth}', was '{zeroth_word}'\")\n",
    "        return\n",
    "    if (final_word := words[-1]) != (expected_final := \"zzyzx\"):\n",
    "        print(f\"Expected final word in words to be '{expected_final}', was '{final_word}'\")\n",
    "        return\n",
    "    print(\"words looks good. Onwards!\")\n",
    "test_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeb1d3b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 1: Generate bigrams\n",
    "\n",
    "Objective: Populate the variable ```bigrams``` with a list of bigrams (2-element tuples) of adjacent characters in ```words```.\n",
    "\n",
    "Treat the start and end of each word as the character '.'\n",
    "\n",
    "Video: [0:06:24](https://youtu.be/PaCmpygFfXo?t=384) and [0:21:55](https://youtu.be/PaCmpygFfXo?t=1315)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65451bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution code\n",
    "bigrams = []\n",
    "for word in words:\n",
    "    bigrams.append(('.', word[0]))\n",
    "    for pos in range(len(word) - 1):\n",
    "        bigrams.append((word[pos], word[pos + 1]))\n",
    "    bigrams.append((word[-1], '.'))\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df32b03",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_bigrams():\n",
    "    if not isinstance(bigrams, list):\n",
    "        print(f\"Expected bigrams to be a list\")\n",
    "        return\n",
    "    if (start_m_ct := bigrams.count(('.', 'm'))) != (expected_start_m_ct := 2538):\n",
    "        print(f\"Expected {expected_start_m_ct} ('a', 'b') bigrams, found {start_m_ct}\")\n",
    "        return\n",
    "    if (ab_ct := bigrams.count(('a', 'b'))) != (expected_ab_ct := 541):\n",
    "        print(f\"Expected {expected_ab_ct} ('a', 'b') bigrams, found {ab_ct}\")\n",
    "        return\n",
    "    if (s_end_ct := bigrams.count(('s', '.'))) != (expected_s_end_ct := 1169):\n",
    "        print(f\"Expected {expected_s_end_ct} ('s', '.') bigrams, found {s_end_ct}\")\n",
    "        return\n",
    "    print(\"bigrams looks good. Onwards!\")\n",
    "test_bigrams()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e707bc",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 2: Map characters to indices\n",
    "\n",
    "Objective: Build a dict ```stoi``` where the key is a character from ```words``` (including '.' for start/end) and the value is a unique integer.\n",
    "\n",
    "(We'll use these unique integers as an index to represent the characters in a Tensor in later steps)\n",
    "\n",
    "Video: [0:15:40](https://youtu.be/PaCmpygFfXo?t=940)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167274f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {}\n",
    "# Solution code\n",
    "chars = set()\n",
    "for bigram in bigrams:\n",
    "    chars.add(bigram[0])\n",
    "    chars.add(bigram[1])\n",
    "stoi = { v:k for (k, v) in enumerate(sorted(chars))}\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a0a35f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def test_stoi():\n",
    "    if not isinstance(stoi, dict):\n",
    "        print(f\"Expected stoi to be a dict\")\n",
    "        return\n",
    "    for c in string.ascii_lowercase:\n",
    "        if not c in stoi:\n",
    "            print(f\"Expected {c} to be in stoi\")\n",
    "            return\n",
    "    print(\"stoi looks good. Onwards!\")\n",
    "test_stoi()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af01aab",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 3: Map indices to characters\n",
    "\n",
    "Objective: Build a dict ```itos``` that has the same key-value pairs as ```stoi```, but with each pair's key and value swapped.\n",
    "\n",
    "Video: [0:18:49](https://youtu.be/PaCmpygFfXo?t=1129)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e05c587",
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = {}\n",
    "# Solution code\n",
    "itos = {stoi[c]:c for c in stoi}\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bb6cc2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_itos():\n",
    "    if not isinstance(itos, dict):\n",
    "        print(f\"Expected itos to be a dict\")\n",
    "        return\n",
    "    if (len_itos := len(itos)) != (expected_len := len(stoi)):\n",
    "        print(f\"Expected length to be {expected_len}, was {len_itos}\")\n",
    "        return\n",
    "    for k,v in stoi.items():\n",
    "        if v not in itos:\n",
    "            print(f\"Expected {v} to be a key in itos\")\n",
    "            return\n",
    "        if (itos_v := itos[v]) != (expected_itos_v := k):\n",
    "            print(f\"Expected itos[{v}] to be {expected_itos_v}, was {itos_v}\")\n",
    "            return\n",
    "    print(\"itos looks good. Onwards!\")\n",
    "test_itos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313392df",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 4: Count occurrences of each bigram\n",
    "\n",
    "Objective: Build a torch Tensor ```N``` where:\n",
    "* the row is the index of the first character in the bigram\n",
    "* the column is the index of the second character in the bigram\n",
    "* the value is the number of times that bigram occurs (represented as an integer)\n",
    "\n",
    "Video: [0:12:45](https://www.youtube.com/watch?v=PaCmpygFfXo&t=1315s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad1186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Solution code\n",
    "N = torch.zeros(len(stoi), len(stoi), dtype=torch.int32)\n",
    "for bigram in bigrams:\n",
    "    i0 = stoi[bigram[0]]\n",
    "    i1 = stoi[bigram[1]]\n",
    "    N[i0][i1] += 1\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fb3b12",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_N():\n",
    "    if torch.is_floating_point(N):\n",
    "        print(f\"Expected N to be a tensor of integral type, was of floating point type.\")\n",
    "        return\n",
    "    if (N_shape := N.shape) != (expected_N_shape := (27, 27)):\n",
    "        print(f\"Expected shape of N to be {expected_N_shape}, was {N_shape}\")\n",
    "        return\n",
    "    if (N_sum := N.sum()) != (expected_N_sum := 228146):\n",
    "        print(f\"Expected the sum of elements in N to be {expected_N_sum}, was {N_sum}\")\n",
    "        return\n",
    "    if (N_start_m := N[stoi['.']][stoi['m']]) != (expected_N_start_m := 2538):\n",
    "        print(f\"Expected N for ('.', 'm') to be {expected_N_start_m}, was {N_start_m}\")\n",
    "        return\n",
    "    if (N_m_end := N[stoi['m']][stoi['.']]) != (expected_N_m_end := 516):\n",
    "        print(f\"Expected N for ('m', '.') to be {expected_N_m_end}, was {N_m_end}\")\n",
    "        return\n",
    "    print(\"N looks good. Onwards!\")\n",
    "test_N()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8760f1",
   "metadata": {},
   "source": [
    "### Step 5: Build probability distribution of bigrams\n",
    "\n",
    "Objective: Build a torch Tensor ```P``` where:\n",
    "* the row is the index of the first character in a bigram\n",
    "* the column is the index of the second character in a bigram\n",
    "* the value is the probability (as torch.float64) of a bigram in ```bigrams``` ending with the second character if it starts with the first character\n",
    "\n",
    "Video: [0:25:35](https://youtu.be/PaCmpygFfXo?t=1535) and [0:36:17](https://youtu.be/PaCmpygFfXo?t=2177)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b199068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution code\n",
    "P = torch.zeros(len(stoi), len(stoi), dtype=torch.float64)\n",
    "N_sum = N.sum(1, keepdim=True)\n",
    "P = N / N_sum\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c75da1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_P():\n",
    "    for row_idx in itos:\n",
    "        if abs(1.0 - (row_sum := P[row_idx].sum().item())) > 0.00001:\n",
    "            row_c = itos[row_idx]\n",
    "            print(f\"Expected the sum of row {row_idx} ({row_c}) to be 1.0, was {row_sum}\")\n",
    "            return\n",
    "    print(\"P looks good. Onwards!\")\n",
    "test_P()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f032a09",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 6: Write a bigram probability calculation function\n",
    "\n",
    "This is slightly different from the steps that the Karpathy video follows, but will make it easier for this worksheet to verify your code.\n",
    "\n",
    "Write a ```bigram_probability``` function that takes the following arguments:\n",
    "* a 2-element tuple of characters\n",
    "\n",
    "And returns:\n",
    "* a floating-point number from 0.0 to 1.0 that represents the probability of the second character following the first character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c416e0a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Solution code\n",
    "def bigram_probability(bigram):\n",
    "    return P[stoi[bigram[0]]][stoi[bigram[1]]]\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c172c0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_bigram_probability():\n",
    "    if (prob_start_end := bigram_probability(('.', '.'))) != (expected_start_end := 0.0):\n",
    "        print(f\"Calculated probability of ('.', '.') is {prob_start_end}, expected {expected_start_end}\")\n",
    "        return\n",
    "    if abs((prob_m_a := bigram_probability(('m', 'a'))) - (expected_m_a := 0.3899)) > 0.001:\n",
    "        print(f\"Calculated probability of ('m', 'a') is {prob_m_a}, expected {expected_m_a}\")\n",
    "        return\n",
    "    print(\"bigram_probability looks good. Onwards!\")\n",
    "test_bigram_probability()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2ea89c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 7: Write a negative log likelihood loss function\n",
    "\n",
    "Write a ```calculate_loss``` function that takes the following arguments:\n",
    "* the name of the bigram probability function written in step 6\n",
    "* a list of bigrams (2-element tuples)\n",
    "\n",
    "And returns:\n",
    "* a floating-point number representing the negative log likelihood of all of the bigrams in the list argument\n",
    "    * Note that Karpathy defines this to be the negative of the *mean* of each tuple's log likelihood, not their sum\n",
    "\n",
    "Video: [0:50:47](https://youtu.be/PaCmpygFfXo?t=3047)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492d1ccf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Solution code\n",
    "import math\n",
    "\n",
    "def calculate_loss(probability_func, bigram_list):\n",
    "    probabilities = list(map(probability_func, bigram_list))\n",
    "    log_probabilities = list(map(math.log, probabilities))\n",
    "    negative_log_likelihood = - sum(log_probabilities) / len(log_probabilities)\n",
    "    return negative_log_likelihood\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cc4900",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_calculate_loss():\n",
    "    bigrams = [('.', 'a'), ('a', 'b'), ('b', '.')]\n",
    "    if abs((all_ones := calculate_loss(lambda _ : 1.0, bigrams)) - (expected_all_ones := 0.0)) > 0.0001:\n",
    "        print(f\"Using a probability_func that always returns 1.0 resulted in {all_ones}, expected {expected_all_ones}\")\n",
    "        return\n",
    "    # TODO: Handle zero-probability tuples somehow.\n",
    "    # if (all_zeroes := calculate_loss(lambda _ : 0.0, bigrams)) != (expected_all_zeroes := math.inf):\n",
    "    #    print(f\"Using a probability_func that always returns 0.0 resulted in {all_zeroes}, expected {expected_all_zeroes} \")\n",
    "    #    return\n",
    "    if abs((using_bp := calculate_loss(bigram_probability, bigrams)) - (expected_using_bp := 3.0881)) > 0.0001:\n",
    "        print(f\"Using your bigram_probability function resulted in {using_bp}, expected {expected_using_bp}\")\n",
    "        return\n",
    "    print(\"calculate_loss looks good. Onwards!\")\n",
    "test_calculate_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863f34a4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 8: Calculate ```bigram_probability```'s loss for the bigrams in ```words```\n",
    "\n",
    "Use the function from step #7 to calculate the bigram probability function's loss when given all of the bigrams in ```words```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4105a47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_for_words = 0.0\n",
    "# Solution code\n",
    "loss_for_words = calculate_loss(bigram_probability, bigrams)\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ca087e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_loss_for_words():\n",
    "    if abs(loss_for_words - (expected_loss := 2.4540)) > 0.0001:\n",
    "        print(f\"loss_for_words is {loss_for_words}, expected {expected_loss}\")\n",
    "        return\n",
    "    print(\"loss_for_words looks good. Congratulations!\")\n",
    "test_loss_for_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d59d7d1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 9: Pick characters based on the probabilities\n",
    "\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* a probability distribution like the one you built in step 5\n",
    "* a ```torch.Generator``` to make the selection process deterministic\n",
    "\n",
    "And returns:\n",
    "* a word (string) generated by repeatedly sampling the probability distribution to determine the next character in the string\n",
    "\n",
    "Video: [0:26:28](https://youtu.be/PaCmpygFfXo?t=1588)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f794820a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Solution code\n",
    "def generate_word(probabilities, generator):\n",
    "    current_letter_index = stoi['.']\n",
    "    word = \"\"\n",
    "    while True:\n",
    "        current_letter_index = torch.multinomial(probabilities[current_letter_index], 1, generator=generator).item()\n",
    "        current_letter_index\n",
    "        if current_letter_index == stoi['.']:\n",
    "            break\n",
    "        word += itos[current_letter_index]\n",
    "    return word\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564e4241",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generate_word():\n",
    "    generator = torch.Generator()\n",
    "    generator.manual_seed(2147483645)\n",
    "    # Colab with pytorch 2 produces different samples with the same seed than local Jupyter Lab/Notebook\n",
    "    # Check for both\n",
    "    expected_words = (\"machina\", \"drlen\")\n",
    "    if not (word := generate_word(P, generator)) in expected_words:\n",
    "        print(f\"Generated word was {word}, expected one of {expected_words}\")\n",
    "        return\n",
    "    print(\"generate_word looks good. Onwards!\")\n",
    "test_generate_word()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab3fca4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
