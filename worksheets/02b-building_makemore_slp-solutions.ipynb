{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ca2e5d6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "\n",
    "# Worksheet 2b - Single-Layer Perceptron\n",
    "\n",
    "This is the third in a series of companion worksheets for for Andrej Karpathy's [Neural Networks: Zero To Hero](https://karpathy.ai/zero-to-hero.html) videos.\n",
    "\n",
    "It corresponds to roughly the second half of the second video in the series, named \"[The spelled-out intro to language modeling: building makemore](https://www.youtube.com/watch?v=PaCmpygFfXo)\".\n",
    "\n",
    "The rest of the worksheets are listed in the README [here](https://github.com/Russ741/karpathy-nn-z2h/).\n",
    "\n",
    "The overall objective of this worksheet is to write code that generates a word that is similar to a set of example words it is trained on.\n",
    "It does so using a single-layer neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10ffc9e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Prerequisite: Load worksheet utilities and download word list\n",
    "\n",
    "The following cell imports [utility functions](https://github.com/Russ741/karpathy-nn-z2h/blob/main/worksheets/worksheet_utils.py) that this worksheet depends on.\n",
    "If the file isn't already locally available (e.g. for Colab), it downloads it from GitHub.\n",
    "\n",
    "Similarly, if this directory does not already contain names.txt, it downloads it from\n",
    "[the makemore GitHub repository](https://github.com/karpathy/makemore/blob/master/names.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00feead1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import shutil\n",
    "\n",
    "try:\n",
    "    from worksheet_utils import *\n",
    "    print(\"worksheet_utils found.\")\n",
    "except ModuleNotFoundError:\n",
    "    utils_local_filename = \"worksheet_utils.py\"\n",
    "    print(f\"Downloading worksheet_utils.\")\n",
    "    with urllib.request.urlopen(\"https://raw.githubusercontent.com/Russ741/karpathy-nn-z2h/main/worksheets/worksheet_utils.py\") as response:\n",
    "        with open(utils_local_filename, mode=\"xb\") as utils_file:\n",
    "            shutil.copyfileobj(response, utils_file)\n",
    "            from worksheet_utils import *\n",
    "\n",
    "WORDS_PATH = \"names.txt\"\n",
    "if os.path.isfile(WORDS_PATH):\n",
    "    print(\"word file found.\")\n",
    "else:\n",
    "    print(\"word file not found, downloading.\")\n",
    "    with urllib.request.urlopen(\"https://github.com/karpathy/makemore/raw/master/names.txt\") as response:\n",
    "        with open(WORDS_PATH, mode=\"xb\") as words_file:\n",
    "            shutil.copyfileobj(response, words_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7113d197",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Preamble: Load data\n",
    "\n",
    "Objective: Write a function that:\n",
    " * Returns a list of strings\n",
    "   * Each string should be equal to the word from the corresponding line of the word file (at ```WORDS_PATH```)\n",
    "   * The strings should not include line-break characters\n",
    "\n",
    "Note: In practice, the order of the strings in the returned list does not matter, but for the\n",
    "test to pass, they should be in the same order in the list as in the word file.\n",
    "\n",
    "Video: [0:03:03](https://youtu.be/PaCmpygFfXo?t=183)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45db16ed",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_words():\n",
    "# Solution code\n",
    "    words = open(WORDS_PATH).read().splitlines()\n",
    "    return words\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efa3cb5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_words():\n",
    "    expect_type(\"loaded_words\", loaded_words, list)\n",
    "    expect_eq(\"len(loaded_words)\", len(loaded_words), 32033)\n",
    "    expect_eq(\"loaded_words[0]\", loaded_words[0], \"emma\")\n",
    "    expect_eq(\"loaded_words[-1]\", loaded_words[-1], \"zzyzx\")\n",
    "    print(\"load_words looks good. Onwards!\")\n",
    "loaded_words = load_words()\n",
    "test_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5828b9ec",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 1: Generate bigrams\n",
    "\n",
    "Objective: Populate the variable ```bigrams``` with a list of bigrams (2-element tuples) of adjacent characters in ```words```.\n",
    "\n",
    "Treat the start and end of each word as the character '.'\n",
    "\n",
    "Video: [0:06:24](https://youtu.be/PaCmpygFfXo?t=384) and [0:21:55](https://youtu.be/PaCmpygFfXo?t=1315)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723987a4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generate_bigrams(words):\n",
    "# Solution code\n",
    "    bigrams = []\n",
    "    for word in words:\n",
    "        bigrams.append(('.', word[0]))\n",
    "        for pos in range(len(word) - 1):\n",
    "            bigrams.append((word[pos], word[pos + 1]))\n",
    "        bigrams.append((word[-1], '.'))\n",
    "    return bigrams\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b26e5d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_generate_bigrams():\n",
    "    bigrams = generate_bigrams(loaded_words)\n",
    "    expect_type(\"bigrams\", bigrams, list)\n",
    "    expect_eq(\"count of ('.', 'm') bigrams\", bigrams.count(('.', 'm')), 2538)\n",
    "    expect_eq(\"count of ('a', 'b') bigrams\", bigrams.count(('a', 'b')), 541)\n",
    "    expect_eq(\"count of ('s', '.') bigrams\", bigrams.count(('s', '.')), 1169)\n",
    "    print(\"generate_bigrams looks good. Onwards!\")\n",
    "test_generate_bigrams()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de01f08a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 2: Map characters to indices\n",
    "\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* a list of char, char tuples representing all of the bigrams in a word list\n",
    "\n",
    "And returns:\n",
    "* a dict (```stoi```) where\n",
    "  * the key is a character from ```words``` (including '.' for start/end),\n",
    "  * the value is a unique integer, and\n",
    "  * all the values are in the range from 0 to ```len(stoi) - 1``` (no gaps)\n",
    "\n",
    "We'll use these unique integers as an index to represent the characters in a Tensor in later steps\n",
    "\n",
    "Note that for this list of words, the same value of ```stoi``` could be generated without looking at the words at all,\n",
    "but simply by using all the lowercase letters and a period. This approach would be more efficient for this exercise,\n",
    "but will not generalize well conceptually to more complex models in future exercises.\n",
    "\n",
    "Video: [0:15:40](https://youtu.be/PaCmpygFfXo?t=940)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff8fc52",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_stoi(bigrams):\n",
    "# Solution code\n",
    "    chars = set()\n",
    "    for bigram in bigrams:\n",
    "        chars.add(bigram[0])\n",
    "        chars.add(bigram[1])\n",
    "    stoi = { v:k for (k, v) in enumerate(sorted(chars))}\n",
    "    return stoi\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec719ff",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def test_get_stoi():\n",
    "    bigrams = [\n",
    "        ('.', 'h'),\n",
    "        ('h', 'i'),\n",
    "        ('i', '.'),\n",
    "        ('.', 'b'),\n",
    "        ('b', 'y'),\n",
    "        ('y', 'e'),\n",
    "        ('e', '.'),\n",
    "    ]\n",
    "    expected_s = sorted(['.', 'h', 'i', 'b', 'y', 'e'])\n",
    "    stoi = get_stoi(bigrams)\n",
    "    expect_type(\"stoi\", stoi, dict)\n",
    "    s = sorted(stoi.keys())\n",
    "    expect_eq(\"stoi keys when sorted\", s, expected_s)\n",
    "    expected_i = list(range(len(s)))\n",
    "    i = sorted(stoi.values())\n",
    "    expect_eq(\"stoi values when sorted\", i, expected_i)\n",
    "    print(\"get_stoi looks good. Onwards!\")\n",
    "test_get_stoi()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13bd6d6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 3: Map indices to characters\n",
    "\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* a dict (```stoi```) as defined in step 2\n",
    "\n",
    "And returns:\n",
    "* a dict (```itos```) where ```itos``` contains the same key-value pairs as ```stoi``` but with keys and values swapped.\n",
    "\n",
    "E.g. if ```stoi == {'.' : 0, 'b' : 1, 'z', 2}```, then ```itos == {0 : '.', 1 : 'b', 2 : 'z'}```\n",
    "\n",
    "Video: [0:18:49](https://youtu.be/PaCmpygFfXo?t=1129)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c038c6cd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_itos(stoi):\n",
    "# Solution code\n",
    "    itos = {stoi[c]:c for c in stoi}\n",
    "    return itos\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9453e0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def test_get_itos():\n",
    "    stoi = {elem:idx for idx, elem in enumerate(string.ascii_lowercase + \".\")}\n",
    "    itos = get_itos(stoi)\n",
    "    expect_type(\"itos\", itos, dict)\n",
    "    for c in string.ascii_lowercase + \".\":\n",
    "        c_i = stoi[c]\n",
    "        expect_eq(f\"itos.get({c_i})\", itos.get(c_i), c)\n",
    "    print(\"get_itos looks good. Onwards!\")\n",
    "test_get_itos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6558766",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 4: Split bigrams into inputs and outputs\n",
    "\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* a list ```bigrams``` as defined in step 1, and\n",
    "* a dict ```stoi``` as defined in step 2\n",
    "\n",
    "And returns:\n",
    "* a one-dimensional torch.Tensor ```x``` with all of the first characters in the tuples in ```bigrams```\n",
    "* a one-dimensional torch.Tensor ```y``` with all of the second characters in the tuples in ```bigrams```\n",
    "* Note: Both output tensors should be the same length as ```bigrams```\n",
    "\n",
    "Video: [1:05:25](https://youtu.be/PaCmpygFfXo?t=3925)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c218ec",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_x_and_y(bigrams, stoi):\n",
    "# Solution code\n",
    "    x = torch.tensor(list(map(lambda bigram : stoi[bigram[0]], bigrams)))\n",
    "    y = torch.tensor(list(map(lambda bigram : stoi[bigram[-1]], bigrams)))\n",
    "\n",
    "    return x, y\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4895508d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_get_x_and_y():\n",
    "    bigrams = [\n",
    "        ('.', 'h'),\n",
    "        ('h', 'i'),\n",
    "        ('i', '.'),\n",
    "        ('.', 'b'),\n",
    "        ('b', 'y'),\n",
    "        ('y', 'e'),\n",
    "        ('e', '.'),\n",
    "    ]\n",
    "    stoi = {\n",
    "        '.': 0,\n",
    "        'h': 1,\n",
    "        'i': 2,\n",
    "        'b': 3,\n",
    "        'y': 4,\n",
    "        'e': 5,\n",
    "    }\n",
    "    x, y = get_x_and_y(bigrams, stoi)\n",
    "    expect_eq(\"x[0]\", x[0], 0)\n",
    "    expect_eq(\"y[0]\", y[0], 1)\n",
    "    expect_eq(\"x[-2]\", x[-2], 4)\n",
    "    expect_eq(\"y[-2]\", y[-2], 5)\n",
    "    print(\"get_x_and_y looks good. Onwards!\")\n",
    "test_get_x_and_y()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff36ee9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 5: Provide initial values for the model parameters\n",
    "\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* a dict ```stoi``` as defined in step 2\n",
    "  * the length of ```stoi``` will be referred to as ```stoi_n```\n",
    "* a ```torch.Generator``` (```gen```) to provide (pseudo)random initial values for the parameters\n",
    "\n",
    "And returns:\n",
    "* a pytorch.Tensor ```W``` of shape (```stoi_n```, ```stoi_n```) where each element is randomly generated\n",
    "* a pytorch.Tensor ```b``` of shape (1, ```stoi_n```)\n",
    "  * The elements of ```b``` can be zero\n",
    "\n",
    "Video: [1:14:03](https://youtu.be/PaCmpygFfXo?t=4433)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5789e69",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def initialize_w_b(stoi, gen):\n",
    "# Solution code\n",
    "    stoi_n = len(stoi)\n",
    "    W = torch.rand((stoi_n,stoi_n), generator=gen, dtype=torch.float64, requires_grad=True)\n",
    "    b = torch.zeros((1,stoi_n), dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "    return W, b\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5d9a48",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_initialize_w_b():\n",
    "    stoi = {'q': 0, 'w': 1, 'e': 2, 'r': 3}\n",
    "    expected_s_ct = 4\n",
    "    gen = torch.Generator()\n",
    "    gen.manual_seed(12345)\n",
    "    W, b = initialize_w_b(stoi, gen)\n",
    "    expect_eq(\"len(W)\", len(W), expected_s_ct)\n",
    "    for row_idx in range(len(W)):\n",
    "        expect_eq(f\"len(W[{row_idx}])\", len(W[row_idx]), expected_s_ct)\n",
    "        for col_idx in range(len(W[row_idx])):\n",
    "            if (val := W[row_idx][col_idx]) == 0.0:\n",
    "                raise Exception(f\"Expected W[{row_idx}][{col_idx}] to be non-zero.\")\n",
    "    expect_eq(\"W.requires_grad\", W.requires_grad, True)\n",
    "    expect_eq(\"b.requires_grad\", b.requires_grad, True)\n",
    "    expect_eq(\"b.shape\", b.shape, (1, expected_s_ct))\n",
    "    print(\"initialize_w_b looks good. Onwards!\")\n",
    "test_initialize_w_b()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ab8275",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 6: Forward propagation\n",
    "\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* a pytorch.Tensor ```x``` of training or testing inputs\n",
    "* pytorch.Tensors ```W``` and ```b``` representing the parameters of the model\n",
    "\n",
    "And returns:\n",
    "* a pytorch.Tensor ```y_hat``` of the model's predicted outputs for each input in x\n",
    "  * The predicted outputs for a given sample should sum to 1.0\n",
    "  * The shape of ```y_hat``` should be (```len(x)```, ```len(W)```)\n",
    "    * Note that ```len(W)``` represents the number of different characters in the word list\n",
    "\n",
    "Video: [1:15:12](https://youtu.be/PaCmpygFfXo?t=4512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea69c5f6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def forward_prop(x, W, b):\n",
    "# Solution code\n",
    "    one_hot = torch.nn.functional.one_hot(x, len(W)).double()\n",
    "    output = torch.matmul(one_hot, W) + b\n",
    "\n",
    "    softmax = output.exp()\n",
    "    softmax = softmax / softmax.sum(1, keepdim=True)\n",
    "\n",
    "    return softmax\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3199e7cb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_forward_prop():\n",
    "    x = torch.tensor([\n",
    "        1,\n",
    "        0,\n",
    "    ])\n",
    "\n",
    "    W = torch.tensor([\n",
    "        [0.1, 0.9, 0.2, 0.01],\n",
    "        [0.04, 0.2, 1.6, 0.25],\n",
    "        [0.02, 0.03, 0.7, 0.01],\n",
    "    ], dtype=torch.float64)\n",
    "\n",
    "    b = torch.tensor([\n",
    "        0.01, 0.02, 0.03, 0.04\n",
    "    ], dtype=torch.float64)\n",
    "\n",
    "    expected_y_hat = torch.tensor([\n",
    "        [0.1203, 0.1426, 0.5841, 0.1530],\n",
    "        [0.1881, 0.4228, 0.2120, 0.1771],\n",
    "    ], dtype=torch.float64)\n",
    "\n",
    "    y_hat = forward_prop(x, W, b)\n",
    "\n",
    "    expect_tensor_close(\"y_hat for test case\", y_hat, expected_y_hat)\n",
    "    print(\"forward_prop looks good. Onwards!\")\n",
    "test_forward_prop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540ed9eb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 0
   },
   "source": [
    "### Step 7: Loss calculation\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* a pytorch.Tensor ```y_hat``` of predicted outputs for a particular set of inputs\n",
    "* a pytorch.Tensor ```y``` of actual outputs for the same set of inputs\n",
    "\n",
    "And returns:\n",
    "* a floating-point value representing the model's negative log likelihood loss for that set of inputs\n",
    "\n",
    "Video: [1:35:49](https://youtu.be/PaCmpygFfXo&t=5749)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d364aa60",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def calculate_loss(y_hat, y):\n",
    "# Solution code\n",
    "    match_probabilities = y_hat[torch.arange(len(y)), y]\n",
    "    neg_log_likelihood = -match_probabilities.log().mean()\n",
    "    return neg_log_likelihood\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286e04bc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "def test_calculate_loss():\n",
    "    y = torch.tensor([2], dtype=torch.int64)\n",
    "    y_hat = torch.tensor([\n",
    "        [0.0, 0.0, 1.0, 0.0]\n",
    "    ])\n",
    "    expect_close(\"loss for first example\", calculate_loss(y_hat, y), 0.0)\n",
    "\n",
    "    y = torch.tensor([2, 0], dtype=torch.int64)\n",
    "    y_hat = torch.tensor([\n",
    "        [0.09, 0.09, exp(-0.5), 0.09],\n",
    "        [exp(-0.1), 0.01, 0.02, 0.03]\n",
    "    ])\n",
    "    expect_close(\"loss for second example\", calculate_loss(y_hat, y), 0.3)\n",
    "    print(\"calculate_loss looks good. Onwards!\")\n",
    "test_calculate_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea94813",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 8: Gradient descent\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* pytorch.Tensors ```W``` and ```b``` representing the parameters of the model\n",
    "* a floating-point value ```learning_rate``` representing the overall size of adjustment to make to the parameters\n",
    "\n",
    "And returns:\n",
    "* the updated pytorch.Tensors ```W``` and ```b```\n",
    "  * Note: Updating the parameters in-place is desirable, but for ease of testing, please return them regardless.\n",
    "\n",
    "Video: [1:41:26](https://youtu.be/PaCmpygFfXo?t=6086)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07b43c1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def descend_gradient(W, b, learning_rate):\n",
    "# Solution code\n",
    "    W.data -= learning_rate * W.grad\n",
    "    b.data -= learning_rate * b.grad\n",
    "    return W, b\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a303744",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_descend_gradient():\n",
    "    W = torch.tensor([\n",
    "        [1.0, 2.0,],\n",
    "        [3.0, -4.0],\n",
    "        [-5.0, 6.0],\n",
    "    ])\n",
    "    W.grad = torch.tensor([\n",
    "        [-2.0, 1.0],\n",
    "        [0.0, -2.0],\n",
    "        [4.0, 1.0]\n",
    "    ])\n",
    "    b = torch.tensor([\n",
    "        1.0,\n",
    "        2.0,\n",
    "    ])\n",
    "    b.grad = torch.tensor([\n",
    "        -1.0,\n",
    "        0.5,\n",
    "    ])\n",
    "\n",
    "    new_w, new_b = descend_gradient(W, b, 3.0)\n",
    "\n",
    "    expected_new_w = torch.tensor([\n",
    "        [7.0, -1.0],\n",
    "        [3.0, 2.0],\n",
    "        [-17.0, 3.0]\n",
    "    ])\n",
    "    expect_tensor_close(\"new W for test case\", new_w, expected_new_w)\n",
    "\n",
    "    expected_new_b = torch.tensor([\n",
    "        4.0,\n",
    "        0.5,\n",
    "    ])\n",
    "    expect_tensor_close(\"new b for test case\", new_b, expected_new_b)\n",
    "    print(\"descend_gradient looks good. Onward!\")\n",
    "test_descend_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c56fff7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 9: Train model\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* pytorch.Tensors ```x``` and ```y``` as described in Step 4\n",
    "* pytorch.Tensors ```W``` and ```b``` as described in Step 5\n",
    "* a floating-point value ```learning_rate``` representing the overall size of adjustment to make to the parameters\n",
    "\n",
    "Updates the values of W and b to fit the data slightly better\n",
    "\n",
    "And returns:\n",
    "* the loss as defined in Step 6\n",
    "\n",
    "Implementation note: this function should make use of several of the functions you've previously implemented.\n",
    "\n",
    "Video: [1:42:55](https://youtu.be/PaCmpygFfXo?t=6175)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca9a159",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_model(x, y, W, b, learning_rate):\n",
    "# Solution code\n",
    "    y_hat = forward_prop(x,W,b)\n",
    "    loss = calculate_loss(y_hat, y)\n",
    "    W.grad = None\n",
    "    b.grad = None\n",
    "    loss.backward()\n",
    "    W, b = descend_gradient(W, b, learning_rate)\n",
    "    return loss.item()\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25025934",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_train_model():\n",
    "    x = torch.tensor([\n",
    "        0,\n",
    "        1,\n",
    "        2,\n",
    "    ])\n",
    "    y = torch.tensor([\n",
    "        1,\n",
    "        2,\n",
    "        0,\n",
    "    ])\n",
    "    W = torch.tensor([\n",
    "        [1.0, 1.0, 1.0],\n",
    "        [1.0, 1.0, 1.0],\n",
    "        [1.0, 1.0, 1.0],\n",
    "    ], dtype=torch.float64, requires_grad=True)\n",
    "    b = torch.tensor([\n",
    "        0.1,\n",
    "        0.2,\n",
    "        0.3,\n",
    "    ], dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "    loss = train_model(x, y, W, b, 2.0)\n",
    "\n",
    "    expected_W = torch.tensor([\n",
    "        [0.7996, 1.4452, 0.7552],\n",
    "        [0.7996, 0.7785, 1.4219],\n",
    "        [1.4663, 0.7785, 0.7552]\n",
    "    ], dtype=torch.float64)\n",
    "    expect_tensor_close(\"W for test case\", W, expected_W)\n",
    "\n",
    "    expected_b = torch.tensor([\n",
    "        0.1654,\n",
    "        0.2022,\n",
    "        0.2323\n",
    "    ], dtype=torch.float64)\n",
    "    expect_tensor_close(\"b for test case\", b, expected_b)\n",
    "    print(\"train_model looks good. Onward!\")\n",
    "test_train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9215a7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 10: Generate words\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* pytorch.Tensors ```W``` and ```b``` as described in Step 5\n",
    "* a dict ```stoi``` as described in Step 2\n",
    "* a dict ```itos``` as described in Step 3\n",
    "* a torch.Generator to use for pseudorandom selection of elements\n",
    "\n",
    "Repeatedly generates a probability distribution for the next letter to select given the last letter\n",
    "\n",
    "And returns\n",
    "* a string representing a word generated by repeatedly sampling the probability distribution\n",
    "\n",
    "Video: [1:54:31](https://youtu.be/PaCmpygFfXo?t=6871)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b817f163",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generate_word(W, b, stoi, itos, gen):\n",
    "# Solution code\n",
    "    chr = '.'\n",
    "    word = \"\"\n",
    "    while True:\n",
    "        x = torch.tensor([stoi[chr]])\n",
    "        probability_distribution = forward_prop(x, W, b)\n",
    "        sample = torch.multinomial(probability_distribution, 1, generator=gen).item()\n",
    "        chr = itos[sample]\n",
    "        if chr == '.':\n",
    "            break\n",
    "        word += chr\n",
    "    return word\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7eaad9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_generate_word():\n",
    "    stoi = {\n",
    "        '.': 0,\n",
    "        'o': 1,\n",
    "        'n': 2,\n",
    "        'w': 3,\n",
    "        'a': 4,\n",
    "        'r': 5,\n",
    "        'd': 6,\n",
    "    }\n",
    "    stoi_n = len(stoi)\n",
    "    itos = {v:k for k,v in stoi.items()}\n",
    "\n",
    "    W = torch.zeros((stoi_n, stoi_n), dtype=torch.float64)\n",
    "    b = torch.zeros((1, stoi_n), dtype=torch.float64)\n",
    "    # These weights result in a probability distribution where the desired next letter is roughly\n",
    "    # 1000x as likely as the others.\n",
    "    for i in range(stoi_n - 1):\n",
    "        W[i][i+1] = 10.0\n",
    "    W[stoi_n - 1][0] = 10.0\n",
    "\n",
    "    gen = torch.Generator()\n",
    "    gen.manual_seed(2147476727)\n",
    "    expect_eq(\"generate_word result for test case\", generate_word(W, b, stoi, itos, gen), \"onward\")\n",
    "    print(f\"generate_word looks good. Onward!\")\n",
    "test_generate_word()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc44d49",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Finale: Put it all together\n",
    "\n",
    "Objective: Write (and call) a function that:\n",
    "* generates the bigrams and character maps\n",
    "* repeatedly trains the model until its loss is acceptably small\n",
    "  * For reference, the \"perfect\" loss of the probability table approach is approximately 2.4241\n",
    "* uses the model to generate some made-up names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9488c18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution code\n",
    "def train_model_and_generate_words():\n",
    "    bigrams = generate_bigrams(loaded_words)\n",
    "    stoi = get_stoi(bigrams)\n",
    "    itos = get_itos(stoi)\n",
    "    x, y = get_x_and_y(bigrams, stoi)\n",
    "    gen = torch.Generator()\n",
    "    W, b = initialize_w_b(stoi, gen)\n",
    "    for i in range(1, 101, 1):\n",
    "        loss = train_model(x, y, W, b, 10.0)\n",
    "    print(f\"Final loss is {loss}\")\n",
    "    for i in range(10):\n",
    "        print(generate_word(W, b, stoi, itos, gen))\n",
    "train_model_and_generate_words()\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece100b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
