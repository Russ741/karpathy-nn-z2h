{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0736e532",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "\n",
    "# Worksheet 2a - Probability Table\n",
    "\n",
    "This is the second in a series of companion worksheets for for Andrej Karpathy's [Neural Networks: Zero To Hero](https://karpathy.ai/zero-to-hero.html) videos.\n",
    "\n",
    "It corresponds to roughly the first half of the second video in the series, named \"[The spelled-out intro to language modeling: building makemore](https://www.youtube.com/watch?v=PaCmpygFfXo)\".\n",
    "\n",
    "The first worksheet in the series is provided by Andrej, and can be found [here](https://colab.research.google.com/drive/1FPTx1RXtBfc4MaTkf7viZZD4U2F9gtKN?usp=sharing).\n",
    "\n",
    "The rest of the worksheets are listed in the README [here](https://github.com/Russ741/karpathy-nn-z2h/).\n",
    "\n",
    "The overall objective of this worksheet is to write code that generates a word that is similar to a set of example words it is trained on.\n",
    "\n",
    "Note that this worksheet uses a probability table, *not* neural networks like subsequent neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abf71e3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Prerequisite: Load worksheet utilities\n",
    "\n",
    "The following cell imports [utility functions](https://github.com/Russ741/karpathy-nn-z2h/blob/main/worksheets/worksheet_utils.py) that this worksheet depends on.\n",
    "If the file isn't already locally available (e.g. for Colab), it downloads it from GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355dc6af",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  from worksheet_utils import *\n",
    "except ModuleNotFoundError:\n",
    "  import requests\n",
    "\n",
    "  utils_url = \"https://raw.githubusercontent.com/Russ741/karpathy-nn-z2h/main/worksheets/worksheet_utils.py\"\n",
    "  utils_local_filename = \"worksheet_utils.py\"\n",
    "\n",
    "  response = requests.get(utils_url)\n",
    "  with open(utils_local_filename, mode='wb') as localfile:\n",
    "    localfile.write(response.content)\n",
    "\n",
    "  from worksheet_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313cf76b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Preamble: Load data\n",
    "\n",
    "Objective: Load a list of words from the remotely-hosted [names.txt](https://github.com/karpathy/makemore/blob/master/names.txt) file\n",
    "([raw link](https://github.com/karpathy/makemore/raw/master/names.txt)) into a list variable named ```words```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38442571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76077ae",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_words():\n",
    "    expect_type(\"words\", words, list)\n",
    "    expect_eq(\"len(words)\", len(words), 32033)\n",
    "    expect_eq(\"words[0]\", words[0], \"emma\")\n",
    "    expect_eq(\"words[-1]\", words[-1], \"zzyzx\")\n",
    "    print(\"words looks good. Onwards!\")\n",
    "test_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb49975b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 1: Generate bigrams\n",
    "\n",
    "Objective: Populate the variable ```bigrams``` with a list of bigrams (2-element tuples) of adjacent characters in ```words```.\n",
    "\n",
    "Treat the start and end of each word as the character '.'\n",
    "\n",
    "Video: [0:06:24](https://youtu.be/PaCmpygFfXo?t=384) and [0:21:55](https://youtu.be/PaCmpygFfXo?t=1315)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d928635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = []\n",
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15537c49",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_bigrams():\n",
    "    expect_type(\"bigrams\", bigrams, list)\n",
    "    expect_eq(\"count of ('.', 'm') bigrams\", bigrams.count(('.', 'm')), 2538)\n",
    "    expect_eq(\"count of ('a', 'b') bigrams\", bigrams.count(('a', 'b')), 541)\n",
    "    expect_eq(\"count of ('s', '.') bigrams\", bigrams.count(('s', '.')), 1169)\n",
    "    print(\"bigrams looks good. Onwards!\")\n",
    "test_bigrams()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db8c22c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 2: Map characters to indices\n",
    "\n",
    "Objective: Build a dict ```stoi``` where the key is a character from ```words``` (including '.' for start/end) and the value is a unique integer.\n",
    "\n",
    "(We'll use these unique integers as an index to represent the characters in a Tensor in later steps)\n",
    "\n",
    "Video: [0:15:40](https://youtu.be/PaCmpygFfXo?t=940)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cb8bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {}\n",
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70742246",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def test_stoi():\n",
    "    expect_type(\"stoi\", stoi, dict)\n",
    "    for c in string.ascii_lowercase:\n",
    "        if not c in stoi:\n",
    "            print(f\"Expected {c} to be in stoi\")\n",
    "            return\n",
    "    print(\"stoi looks good. Onwards!\")\n",
    "test_stoi()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5962af37",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 3: Map indices to characters\n",
    "\n",
    "Objective: Build a dict ```itos``` that has the same key-value pairs as ```stoi```, but with each pair's key and value swapped.\n",
    "\n",
    "Video: [0:18:49](https://youtu.be/PaCmpygFfXo?t=1129)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bddc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = {}\n",
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22331c69",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_itos():\n",
    "    expect_type(\"itos\", itos, dict)\n",
    "    if (len_itos := len(itos)) != (expected_len := len(stoi)):\n",
    "        print(f\"Expected length to be {expected_len}, was {len_itos}\")\n",
    "        return\n",
    "    expect_eq(\"len(itos)\", len(itos), len(stoi))\n",
    "    for k,v in stoi.items():\n",
    "        if v not in itos:\n",
    "            print(f\"Expected {v} to be a key in itos\")\n",
    "            return\n",
    "        expect_eq(f\"itos[{v}]\", itos[v], k)\n",
    "    print(\"itos looks good. Onwards!\")\n",
    "test_itos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68f8890",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 4: Count occurrences of each bigram\n",
    "\n",
    "Objective: Build a torch Tensor ```N``` where:\n",
    "* the row is the index of the first character in the bigram\n",
    "* the column is the index of the second character in the bigram\n",
    "* the value is the number of times that bigram occurs (represented as an integer)\n",
    "\n",
    "Video: [0:12:45](https://www.youtube.com/watch?v=PaCmpygFfXo&t=1315s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7302743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905094d9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_N():\n",
    "    if torch.is_floating_point(N):\n",
    "        print(f\"Expected N to be a tensor of integral type, was of floating point type.\")\n",
    "        return\n",
    "    expect_eq(\"N.shape\", N.shape, (27, 27))\n",
    "    expect_eq(\"N.sum()\", N.sum(), 228146)\n",
    "    expect_eq(\"N for ('.', 'm')\", N[stoi['.']][stoi['m']], 2538)\n",
    "    expect_eq(\"N for ('m', '.')\", N[stoi['m']][stoi['.']], 516)\n",
    "    print(\"N looks good. Onwards!\")\n",
    "test_N()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b3eb85",
   "metadata": {},
   "source": [
    "### Step 5: Build probability distribution of bigrams\n",
    "\n",
    "Objective: Build a torch Tensor ```P``` where:\n",
    "* the row is the index of the first character in a bigram\n",
    "* the column is the index of the second character in a bigram\n",
    "* the value is the probability (as torch.float64) of a bigram in ```bigrams``` ending with the second character if it starts with the first character\n",
    "\n",
    "Video: [0:25:35](https://youtu.be/PaCmpygFfXo?t=1535) and [0:36:17](https://youtu.be/PaCmpygFfXo?t=2177)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3deebba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc55dd52",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_P():\n",
    "    for row_idx in itos:\n",
    "        row_sum = P[row_idx].sum().item()\n",
    "        expect_close(f\"sum of P[{row_idx}] (for character '{itos[row_idx]}')\", row_sum, 1.0)\n",
    "    print(\"P looks good. Onwards!\")\n",
    "test_P()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4def4a6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 6: Write a bigram probability calculation function\n",
    "\n",
    "This is slightly different from the steps that the Karpathy video follows, but will make it easier for this worksheet to verify your code.\n",
    "\n",
    "Write a ```bigram_probability``` function that takes the following arguments:\n",
    "* a 2-element tuple of characters\n",
    "\n",
    "And returns:\n",
    "* a floating-point number from 0.0 to 1.0 that represents the probability of the second character following the first character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bcd608",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def bigram_probability(bigram):\n",
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196cf73d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_bigram_probability():\n",
    "    expect_eq(\"bigram_probability(('.', '.'))\", bigram_probability(('.', '.')), 0.0)\n",
    "    expect_close(\"bigram_probability(('m', 'a'))\", bigram_probability(('m', 'a')), 0.3899)\n",
    "    print(\"bigram_probability looks good. Onwards!\")\n",
    "test_bigram_probability()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf140e3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 7: Write a negative log likelihood loss function\n",
    "\n",
    "Write a ```calculate_loss``` function that takes the following arguments:\n",
    "* the name of the bigram probability function written in step 6\n",
    "* a list of bigrams (2-element tuples)\n",
    "\n",
    "And returns:\n",
    "* a floating-point number representing the negative log likelihood of all of the bigrams in the list argument\n",
    "    * Note that Karpathy defines this to be the negative of the *mean* of each tuple's log likelihood, not their sum\n",
    "\n",
    "Video: [0:50:47](https://youtu.be/PaCmpygFfXo?t=3047)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffdc4b0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# The sample solution uses this library; if your code doesn't, feel free to remove it.\n",
    "import math\n",
    "\n",
    "def calculate_loss(probability_func, bigram_list):\n",
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4ecbae",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_calculate_loss():\n",
    "    bigrams = [('.', 'a'), ('a', 'b'), ('b', '.')]\n",
    "    expect_close(\"calculate_loss for a probability_func that always returns 1.0\", calculate_loss(lambda _ : 1.0, bigrams), 0.0)\n",
    "    # TODO: Handle zero-probability tuples somehow.\n",
    "    # if (all_zeroes := calculate_loss(lambda _ : 0.0, bigrams)) != (expected_all_zeroes := math.inf):\n",
    "    #    print(f\"Using a probability_func that always returns 0.0 resulted in {all_zeroes}, expected {expected_all_zeroes} \")\n",
    "    #    return\n",
    "    expect_close(\"calculate_loss for bigram_probability\", calculate_loss(bigram_probability, bigrams), 3.0881)\n",
    "    print(\"calculate_loss looks good. Onwards!\")\n",
    "test_calculate_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a16c16e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 8: Calculate ```bigram_probability```'s loss for the bigrams in ```words```\n",
    "\n",
    "Use the function from step #7 to calculate the bigram probability function's loss when given all of the bigrams in ```words```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d591978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_for_words = 0.0\n",
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ca3b77",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_loss_for_words():\n",
    "    expect_close(\"loss_for_words\", loss_for_words, 2.4540)\n",
    "    print(\"loss_for_words looks good. Congratulations!\")\n",
    "test_loss_for_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb09351",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 9: Pick characters based on the probabilities\n",
    "\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* a probability distribution like the one you built in step 5\n",
    "* a ```torch.Generator``` to make the selection process deterministic\n",
    "\n",
    "And returns:\n",
    "* a word (string) generated by repeatedly sampling the probability distribution to determine the next character in the string\n",
    "\n",
    "Video: [0:26:28](https://youtu.be/PaCmpygFfXo?t=1588)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c417dab",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generate_word(probabilities, generator):\n",
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc703a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generate_word():\n",
    "    generator = torch.Generator()\n",
    "    generator.manual_seed(2147483645)\n",
    "    # Colab with pytorch 2 produces different samples with the same seed than local Jupyter Lab/Notebook\n",
    "    # Check for both\n",
    "    expected_words = (\"machina\", \"drlen\")\n",
    "    if not (word := generate_word(P, generator)) in expected_words:\n",
    "        print(f\"Generated word was {word}, expected one of {expected_words}\")\n",
    "        return\n",
    "    print(\"generate_word looks good. Onwards!\")\n",
    "test_generate_word()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fc23d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
