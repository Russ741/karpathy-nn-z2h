{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "357703cb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "\n",
    "# Worksheet 2b - Single-Layer Perceptron\n",
    "\n",
    "This is the third in a series of companion worksheets for for Andrej Karpathy's [Neural Networks: Zero To Hero](https://karpathy.ai/zero-to-hero.html) videos.\n",
    "\n",
    "It corresponds to roughly the second half of the second video in the series, named \"[The spelled-out intro to language modeling: building makemore](https://www.youtube.com/watch?v=PaCmpygFfXo)\".\n",
    "\n",
    "The rest of the worksheets are listed in the README [here](https://github.com/Russ741/karpathy-nn-z2h/).\n",
    "\n",
    "The overall objective of this worksheet is to write code that generates a word that is similar to a set of example words it is trained on.\n",
    "It does so using a single-layer neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8994ac08",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Prerequisite: Load worksheet utilities\n",
    "\n",
    "The following cell imports [utility functions](https://github.com/Russ741/karpathy-nn-z2h/blob/main/worksheets/worksheet_utils.py) that this worksheet depends on.\n",
    "If the file isn't already locally available (e.g. for Colab), it downloads it from GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226b3f97",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  from worksheet_utils import *\n",
    "except ModuleNotFoundError:\n",
    "  import requests\n",
    "\n",
    "  utils_url = \"https://raw.githubusercontent.com/Russ741/karpathy-nn-z2h/main/worksheets/worksheet_utils.py\"\n",
    "  utils_local_filename = \"worksheet_utils.py\"\n",
    "\n",
    "  response = requests.get(utils_url)\n",
    "  with open(utils_local_filename, mode='wb') as localfile:\n",
    "    localfile.write(response.content)\n",
    "\n",
    "  from worksheet_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd67f70",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Preamble: Load data\n",
    "\n",
    "Objective: Write a function that:\n",
    " * Loads the remotely-hosted [names.txt](https://github.com/karpathy/makemore/blob/master/names.txt) file\n",
    "([raw link](https://github.com/karpathy/makemore/raw/master/names.txt))\n",
    " * Returns a list of strings\n",
    "   * Each string should be equal to the word from the corresponding line of names.txt\n",
    "   * The strings should not include line-break characters\n",
    "\n",
    "Note: In practice, the order of the strings in the returned list does not matter, but for the\n",
    "test to pass, they should be in the same order in the list as in words.txt.\n",
    "\n",
    "Hint: In the video, Karpathy has a local copy of words.txt.<br>\n",
    "One way to fetch words.txt is to use a function from the [requests](https://pypi.org/project/requests/) library.\n",
    "\n",
    "Video: [0:03:03](https://youtu.be/PaCmpygFfXo?t=183)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d27d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a115e8c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_words():\n",
    "    if not isinstance(loaded_words, list):\n",
    "        print(f\"Expected words to be a list\")\n",
    "        return\n",
    "    expect_eq(\"len(loaded_words)\", len(loaded_words), 32033)\n",
    "    expect_eq(\"loaded_words[0]\", loaded_words[0], \"emma\")\n",
    "    expect_eq(\"loaded_words[-1]\", loaded_words[-1], \"zzyzx\")\n",
    "    print(\"load_words looks good. Onwards!\")\n",
    "loaded_words = load_words()\n",
    "test_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b336336",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 1: Generate bigrams\n",
    "\n",
    "Objective: Populate the variable ```bigrams``` with a list of bigrams (2-element tuples) of adjacent characters in ```words```.\n",
    "\n",
    "Treat the start and end of each word as the character '.'\n",
    "\n",
    "Video: [0:06:24](https://youtu.be/PaCmpygFfXo?t=384) and [0:21:55](https://youtu.be/PaCmpygFfXo?t=1315)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74456712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded04123",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_generate_bigrams():\n",
    "    bigrams = generate_bigrams(loaded_words)\n",
    "    if not isinstance(bigrams, list):\n",
    "        print(f\"Expected bigrams to be a list\")\n",
    "        return\n",
    "    expect_eq(\"count of ('.', 'm') bigrams\", bigrams.count(('.', 'm')), 2538)\n",
    "    expect_eq(\"count of ('a', 'b') bigrams\", bigrams.count(('a', 'b')), 541)\n",
    "    expect_eq(\"count of ('s', '.') bigrams\", bigrams.count(('s', '.')), 1169)\n",
    "    print(\"generate_bigrams looks good. Onwards!\")\n",
    "test_generate_bigrams()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fa6640",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 2: Map characters to indices\n",
    "\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* a list of char, char tuples representing all of the bigrams in a word list\n",
    "\n",
    "And returns:\n",
    "* a dict (```stoi```) where\n",
    "  * the key is a character from ```words``` (including '.' for start/end),\n",
    "  * the value is a unique integer, and\n",
    "  * all the values are in the range from 0 to ```len(stoi) - 1``` (no gaps)\n",
    "\n",
    "We'll use these unique integers as an index to represent the characters in a Tensor in later steps\n",
    "\n",
    "Note that for this list of words, the same value of ```stoi``` could be generated without looking at the words at all,\n",
    "but simply by using all the lowercase letters and a period. This approach would be more efficient for this exercise,\n",
    "but will not generalize well conceptually to more complex models in future exercises.\n",
    "\n",
    "Video: [0:15:40](https://youtu.be/PaCmpygFfXo?t=940)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f0b000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457140ca",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def test_get_stoi():\n",
    "    bigrams = [\n",
    "        ('.', 'h'),\n",
    "        ('h', 'i'),\n",
    "        ('i', '.'),\n",
    "        ('.', 'b'),\n",
    "        ('b', 'y'),\n",
    "        ('y', 'e'),\n",
    "        ('e', '.'),\n",
    "    ]\n",
    "    expected_s = sorted(['.', 'h', 'i', 'b', 'y', 'e'])\n",
    "    stoi = get_stoi(bigrams)\n",
    "    if not isinstance(stoi, dict):\n",
    "        print(f\"Expected stoi to be a dict\")\n",
    "        return\n",
    "    s = sorted(stoi.keys())\n",
    "    expect_eq(\"stoi keys when sorted\", s, expected_s)\n",
    "    expected_i = list(range(len(s)))\n",
    "    i = sorted(stoi.values())\n",
    "    expect_eq(\"stoi values when sorted\", i, expected_i)\n",
    "    print(\"get_stoi looks good. Onwards!\")\n",
    "test_get_stoi()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124bf0ba",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 3: Map indices to characters\n",
    "\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* a dict (```stoi```) as defined in step 2\n",
    "\n",
    "And returns:\n",
    "* a dict (```itos```) where ```itos``` contains the same key-value pairs as ```stoi``` but with keys and values swapped.\n",
    "\n",
    "E.g. if ```stoi == {'.' : 0, 'b' : 1, 'z', 2}```, then ```itos == {0 : '.', 1 : 'b', 2 : 'z'}```\n",
    "\n",
    "Video: [0:18:49](https://youtu.be/PaCmpygFfXo?t=1129)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ff4980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dff8c8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def test_get_itos():\n",
    "    stoi = {elem:idx for idx, elem in enumerate(string.ascii_lowercase + \".\")}\n",
    "    itos = get_itos(stoi)\n",
    "    if not isinstance(itos, dict):\n",
    "        print(f\"Expected stoi to be a dict\")\n",
    "        return\n",
    "    for c in string.ascii_lowercase + \".\":\n",
    "        c_i = stoi[c]\n",
    "        expect_eq(f\"itos[{c_i}]\", itos[c_i], c)\n",
    "    print(\"get_itos looks good. Onwards!\")\n",
    "test_get_itos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d225cfd5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 4: Split bigrams into inputs and outputs\n",
    "\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* a list ```bigrams``` as defined in step 1, and\n",
    "* a dict ```stoi``` as defined in step 2\n",
    "\n",
    "And returns:\n",
    "* a one-dimensional torch.Tensor ```x``` with all of the first characters in the tuples in ```bigrams```\n",
    "* a one-dimensional torch.Tensor ```y``` with all of the second characters in the tuples in ```bigrams```\n",
    "* Note: Both output tensors should be the same length as ```bigrams```\n",
    "\n",
    "Video: [1:05:25](https://youtu.be/PaCmpygFfXo?t=3925)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa65044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eedd4b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_get_x_and_y():\n",
    "    bigrams = [\n",
    "        ('.', 'h'),\n",
    "        ('h', 'i'),\n",
    "        ('i', '.'),\n",
    "        ('.', 'b'),\n",
    "        ('b', 'y'),\n",
    "        ('y', 'e'),\n",
    "        ('e', '.'),\n",
    "    ]\n",
    "    stoi = {\n",
    "        '.': 0,\n",
    "        'h': 1,\n",
    "        'i': 2,\n",
    "        'b': 3,\n",
    "        'y': 4,\n",
    "        'e': 5,\n",
    "    }\n",
    "    x, y = get_x_and_y(bigrams, stoi)\n",
    "    expect_eq(\"x[0]\", x[0], 0)\n",
    "    expect_eq(\"y[0]\", y[0], 1)\n",
    "    expect_eq(\"x[-2]\", x[-2], 4)\n",
    "    expect_eq(\"y[-2]\", y[-2], 5)\n",
    "    print(\"get_x_and_y looks good. Onwards!\")\n",
    "test_get_x_and_y()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa68ac9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 5: Provide initial values for the model parameters\n",
    "\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* a dict ```stoi``` as defined in step 2\n",
    "  * the length of ```stoi``` will be referred to as ```stoi_n```\n",
    "* a ```torch.Generator``` (```gen```) to provide (pseudo)random initial values for the parameters\n",
    "\n",
    "And returns:\n",
    "* a pytorch.Tensor ```W``` of shape (```stoi_n```, ```stoi_n```) where each element is randomly generated\n",
    "* a pytorch.Tensor ```b``` of shape (1, ```stoi_n```)\n",
    "  * The elements of ```b``` can be zero\n",
    "\n",
    "Video: [1:14:03](https://youtu.be/PaCmpygFfXo?t=4433)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da9ecd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5317f0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_initialize_w_b():\n",
    "    stoi = {'q': 0, 'w': 1, 'e': 2, 'r': 3}\n",
    "    expected_s_ct = 4\n",
    "    gen = torch.Generator()\n",
    "    gen.manual_seed(12345)\n",
    "    W, b = initialize_w_b(stoi, gen)\n",
    "    expect_eq(\"len(W)\", len(W), expected_s_ct)\n",
    "    for row_idx in range(len(W)):\n",
    "        expect_eq(f\"len(W[{row_idx}])\", len(W[row_idx]), expected_s_ct)\n",
    "        for col_idx in range(len(W[row_idx])):\n",
    "            if (val := W[row_idx][col_idx]) == 0.0:\n",
    "                print(f\"Expected W[{row_idx}][{col_idx}] to be non-zero.\")\n",
    "                return\n",
    "    expect_eq(\"W.requires_grad\", W.requires_grad, True)\n",
    "    expect_eq(\"b.requires_grad\", b.requires_grad, True)\n",
    "    expect_eq(\"b.shape\", b.shape, (1, expected_s_ct))\n",
    "    print(\"initialize_w_b looks good. Onwards!\")\n",
    "test_initialize_w_b()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1d079f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 6: Forward propagation\n",
    "\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* a pytorch.Tensor ```x``` of training or testing inputs\n",
    "* pytorch.Tensors ```W``` and ```b``` representing the parameters of the model\n",
    "\n",
    "And returns:\n",
    "* a pytorch.Tensor ```y_hat``` of the model's predicted outputs for each input in x\n",
    "  * The predicted outputs for a given sample should sum to 1.0\n",
    "  * The shape of ```y_hat``` should be (```len(x)```, ```len(W)```)\n",
    "    * Note that ```len(W)``` represents the number of different characters in the word list\n",
    "\n",
    "Video: [1:15:12](https://youtu.be/PaCmpygFfXo?t=4512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686d9594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d372f0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_forward_prop():\n",
    "    x = torch.tensor([\n",
    "        1,\n",
    "        0,\n",
    "    ])\n",
    "\n",
    "    W = torch.tensor([\n",
    "        [0.1, 0.9, 0.2, 0.01],\n",
    "        [0.04, 0.2, 1.6, 0.25],\n",
    "        [0.02, 0.03, 0.7, 0.01],\n",
    "    ], dtype=torch.float64)\n",
    "\n",
    "    b = torch.tensor([\n",
    "        0.01, 0.02, 0.03, 0.04\n",
    "    ], dtype=torch.float64)\n",
    "\n",
    "    expected_y_hat = torch.tensor([\n",
    "        [0.1203, 0.1426, 0.5841, 0.1530],\n",
    "        [0.1881, 0.4228, 0.2120, 0.1771],\n",
    "    ], dtype=torch.float64)\n",
    "\n",
    "    y_hat = forward_prop(x, W, b)\n",
    "\n",
    "    if not torch.isclose(expected_y_hat, y_hat, rtol = 0.0, atol = 0.0001).all():\n",
    "        print(f\"Expected y_hat for test case to be \\n{expected_y_hat}\\n, was \\n{y_hat}\")\n",
    "        return\n",
    "    print(\"forward_prop looks good. Onwards!\")\n",
    "test_forward_prop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e186fb1b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 0
   },
   "source": [
    "### Step 7: Loss calculation\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* a pytorch.Tensor ```y_hat``` of predicted outputs for a particular set of inputs\n",
    "* a pytorch.Tensor ```y``` of actual outputs for the same set of inputs\n",
    "\n",
    "And returns:\n",
    "* a floating-point value representing the model's negative log likelihood loss for that set of inputs\n",
    "\n",
    "Video: [1:35:49](https://youtu.be/PaCmpygFfXo&t=5749)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febe33b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cf56b0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "def test_calculate_loss():\n",
    "    y = torch.tensor([2], dtype=torch.int64)\n",
    "    y_hat = torch.tensor([\n",
    "        [0.0, 0.0, 1.0, 0.0]\n",
    "    ])\n",
    "    if abs((loss := calculate_loss(y_hat, y))) > 0.00001:\n",
    "        print(f\"Expected loss for first example to be 0.0, was {loss}\")\n",
    "        return\n",
    "\n",
    "    y = torch.tensor([2, 0], dtype=torch.int64)\n",
    "    y_hat = torch.tensor([\n",
    "        [0.09, 0.09, exp(-0.5), 0.09],\n",
    "        [exp(-0.1), 0.01, 0.02, 0.03]\n",
    "    ])\n",
    "    if abs((loss := calculate_loss(y_hat, y)) - (expected_loss := 0.3)) > 0.00001:\n",
    "        print(f\"Expected loss for second example to be {expected_loss}, was {loss}\")\n",
    "        return\n",
    "    print(\"calculate_loss looks good. Onwards!\")\n",
    "test_calculate_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621444d5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 8: Gradient descent\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* pytorch.Tensors ```W``` and ```b``` representing the parameters of the model\n",
    "* a floating-point value ```learning_rate``` representing the overall size of adjustment to make to the parameters\n",
    "\n",
    "And returns:\n",
    "* the updated pytorch.Tensors ```W``` and ```b```\n",
    "  * Note: Updating the parameters in-place is desirable, but for ease of testing, please return them regardless.\n",
    "\n",
    "Video: [1:41:26](https://youtu.be/PaCmpygFfXo?t=6086)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f71727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c63eed7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_descend_gradient():\n",
    "    W = torch.tensor([\n",
    "        [1.0, 2.0,],\n",
    "        [3.0, -4.0],\n",
    "        [-5.0, 6.0],\n",
    "    ])\n",
    "    W.grad = torch.tensor([\n",
    "        [-2.0, 1.0],\n",
    "        [0.0, -2.0],\n",
    "        [4.0, 1.0]\n",
    "    ])\n",
    "    b = torch.tensor([\n",
    "        1.0,\n",
    "        2.0,\n",
    "    ])\n",
    "    b.grad = torch.tensor([\n",
    "        -1.0,\n",
    "        0.5,\n",
    "    ])\n",
    "    new_w, new_b = descend_gradient(W, b, 3.0)\n",
    "    expected_new_w = torch.tensor([\n",
    "        [7.0, -1.0],\n",
    "        [3.0, 2.0],\n",
    "        [-17.0, 3.0]\n",
    "    ])\n",
    "    if not new_w.equal(expected_new_w):\n",
    "        print(f\"Expected new W for test case to be \\n{expected_new_w}\\n, is \\n{new_w}\")\n",
    "        return\n",
    "    expected_new_b = torch.tensor([\n",
    "        4.0,\n",
    "        0.5,\n",
    "    ])\n",
    "    if not new_b.equal(expected_new_b):\n",
    "        print(f\"Expected new b for test case to be \\n{expected_new_b}\\n, is \\n{new_b}\")\n",
    "        return\n",
    "    print(\"descend_gradient looks good. Onward!\")\n",
    "test_descend_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5509668",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 9: Train model\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* pytorch.Tensors ```x``` and ```y``` as described in Step 4\n",
    "* pytorch.Tensors ```W``` and ```b``` as described in Step 5\n",
    "* a floating-point value ```learning_rate``` representing the overall size of adjustment to make to the parameters\n",
    "\n",
    "Updates the values of W and b to fit the data slightly better\n",
    "\n",
    "And returns:\n",
    "* the loss as defined in Step 6\n",
    "\n",
    "Implementation note: this function should make use of several of the functions you've previously implemented.\n",
    "\n",
    "Video: [1:42:55](https://youtu.be/PaCmpygFfXo?t=6175)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d698d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d51e246",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_train_model():\n",
    "    x = torch.tensor([\n",
    "        0,\n",
    "        1,\n",
    "        2,\n",
    "    ])\n",
    "    y = torch.tensor([\n",
    "        1,\n",
    "        2,\n",
    "        0,\n",
    "    ])\n",
    "    W = torch.tensor([\n",
    "        [1.0, 1.0, 1.0],\n",
    "        [1.0, 1.0, 1.0],\n",
    "        [1.0, 1.0, 1.0],\n",
    "    ], dtype=torch.float64, requires_grad=True)\n",
    "    b = torch.tensor([\n",
    "        0.1,\n",
    "        0.2,\n",
    "        0.3,\n",
    "    ], dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "    loss = train_model(x, y, W, b, 2.0)\n",
    "\n",
    "    expected_W = torch.tensor([\n",
    "        [0.7996, 1.4452, 0.7552],\n",
    "        [0.7996, 0.7785, 1.4219],\n",
    "        [1.4663, 0.7785, 0.7552]\n",
    "    ], dtype=torch.float64)\n",
    "    if not torch.isclose(expected_W, W, rtol = 0.0, atol = 0.0001).all():\n",
    "        print(f\"Expected W for test case to be \\n{expected_W}\\n, was \\n{W}\")\n",
    "        return\n",
    "\n",
    "    expected_b = torch.tensor([\n",
    "        0.1654,\n",
    "        0.2022,\n",
    "        0.2323\n",
    "    ], dtype=torch.float64)\n",
    "    if not torch.isclose(expected_b, b, rtol = 0.0, atol = 0.0001).all():\n",
    "        print(f\"Expected b for test case to be \\n{expected_b}\\n, was \\n{b}\")\n",
    "        return\n",
    "    print(\"train_model looks good. Onward!\")\n",
    "test_train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf582849",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 10: Generate words\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* pytorch.Tensors ```W``` and ```b``` as described in Step 5\n",
    "* a dict ```stoi``` as described in Step 2\n",
    "* a dict ```itos``` as described in Step 3\n",
    "* a torch.Generator to use for pseudorandom selection of elements\n",
    "\n",
    "Repeatedly generates a probability distribution for the next letter to select given the last letter\n",
    "\n",
    "And returns\n",
    "* a string representing a word generated by repeatedly sampling the probability distribution\n",
    "\n",
    "Video: [1:54:31](https://youtu.be/PaCmpygFfXo?t=6871)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d75463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3425c382",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_generate_word():\n",
    "    stoi = {\n",
    "        '.': 0,\n",
    "        'o': 1,\n",
    "        'n': 2,\n",
    "        'w': 3,\n",
    "        'a': 4,\n",
    "        'r': 5,\n",
    "        'd': 6,\n",
    "    }\n",
    "    stoi_n = len(stoi)\n",
    "    itos = {v:k for k,v in stoi.items()}\n",
    "\n",
    "    W = torch.zeros((stoi_n, stoi_n), dtype=torch.float64)\n",
    "    b = torch.zeros((1, stoi_n), dtype=torch.float64)\n",
    "    # These weights result in a probability distribution where the desired next letter is roughly\n",
    "    # 1000x as likely as the others.\n",
    "    for i in range(stoi_n - 1):\n",
    "        W[i][i+1] = 10.0\n",
    "    W[stoi_n - 1][0] = 10.0\n",
    "\n",
    "    gen = torch.Generator()\n",
    "    gen.manual_seed(2147476727)\n",
    "    expect_eq(\"generate_word result for test case\", generate_word(W, b, stoi, itos, gen), \"onward\")\n",
    "    print(f\"generate_word looks good. Onward!\")\n",
    "test_generate_word()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c3cbeb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Finale: Put it all together\n",
    "\n",
    "Objective: Write (and call) a function that:\n",
    "* generates the bigrams and character maps\n",
    "* repeatedly trains the model until its loss is acceptably small\n",
    "  * For reference, the \"perfect\" loss of the probability table approach is approximately 2.4241\n",
    "* uses the model to generate some made-up names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfecc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c085814f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
