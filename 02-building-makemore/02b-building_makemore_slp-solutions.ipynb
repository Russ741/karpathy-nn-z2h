{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e28b5b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Import some assertions for the test cases.\n",
    "from utils import expect_close"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7113d197",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Preamble: Load data\n",
    "\n",
    "Objective: Load a list of words from the [names.txt](https://github.com/karpathy/makemore/blob/master/names.txt) file into a list variable named ```words```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45db16ed",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def load_words():\n",
    "    words_url = 'https://raw.githubusercontent.com/karpathy/makemore/master/names.txt'\n",
    "    words = requests.get(words_url).text.splitlines()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efa3cb5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_words():\n",
    "    if not isinstance(loaded_words, list):\n",
    "        print(f\"Expected words to be a list\")\n",
    "        return\n",
    "    if (len_words := len(loaded_words)) != (expected_words := 32033):\n",
    "        print(f\"Expected {expected_words} elements in words, found {len_words} elements\")\n",
    "        return\n",
    "    if (zeroth_word := loaded_words[0]) != (expected_zeroth := \"emma\"):\n",
    "        print(f\"Expected zeroth word in words to be '{expected_zeroth}', was '{zeroth_word}'\")\n",
    "        return\n",
    "    if (final_word := loaded_words[-1]) != (expected_final := \"zzyzx\"):\n",
    "        print(f\"Expected final word in words to be '{expected_final}', was '{final_word}'\")\n",
    "        return\n",
    "    print(\"load_words looks good. Onwards!\")\n",
    "loaded_words = load_words()\n",
    "test_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723987a4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_bigrams(words):\n",
    "    bigrams = []\n",
    "    for word in words:\n",
    "        bigrams.append(('.', word[0]))\n",
    "        for pos in range(len(word) - 1):\n",
    "            bigrams.append((word[pos], word[pos + 1]))\n",
    "        bigrams.append((word[-1], '.'))\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b26e5d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_generate_bigrams():\n",
    "    bigrams = generate_bigrams(loaded_words)\n",
    "    if not isinstance(bigrams, list):\n",
    "        print(f\"Expected bigrams to be a list\")\n",
    "        return\n",
    "    if (start_m_ct := bigrams.count(('.', 'm'))) != (expected_start_m_ct := 2538):\n",
    "        print(f\"Expected {expected_start_m_ct} ('a', 'b') bigrams, found {start_m_ct}\")\n",
    "        return\n",
    "    if (ab_ct := bigrams.count(('a', 'b'))) != (expected_ab_ct := 541):\n",
    "        print(f\"Expected {expected_ab_ct} ('a', 'b') bigrams, found {ab_ct}\")\n",
    "        return\n",
    "    if (s_end_ct := bigrams.count(('s', '.'))) != (expected_s_end_ct := 1169):\n",
    "        print(f\"Expected {expected_s_end_ct} ('s', '.') bigrams, found {s_end_ct}\")\n",
    "        return\n",
    "    print(\"generate_bigrams looks good. Onwards!\")\n",
    "test_generate_bigrams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff8fc52",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_stoi(bigrams):\n",
    "    chars = set()\n",
    "    for bigram in bigrams:\n",
    "        chars.add(bigram[0])\n",
    "        chars.add(bigram[1])\n",
    "    stoi = { v:k for (k, v) in enumerate(sorted(chars))}\n",
    "    return stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec719ff",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def test_get_stoi():\n",
    "    bigrams = [\n",
    "        ('.', 'h'),\n",
    "        ('h', 'i'),\n",
    "        ('i', '.'),\n",
    "        ('.', 'b'),\n",
    "        ('b', 'y'),\n",
    "        ('y', 'e'),\n",
    "        ('e', '.'),\n",
    "    ]\n",
    "    expected_s = sorted(['.', 'h', 'i', 'b', 'y', 'e'])\n",
    "    stoi = get_stoi(bigrams)\n",
    "    if not isinstance(stoi, dict):\n",
    "        print(f\"Expected stoi to be a dict\")\n",
    "        return\n",
    "    s = sorted(stoi.keys())\n",
    "    if s != expected_s:\n",
    "        print(f\"Expected stoi keys to be {expected_s} when sorted, were {s}\")\n",
    "        return\n",
    "    expected_i = list(range(len(s)))\n",
    "    i = sorted(stoi.values())\n",
    "    if i != expected_i:\n",
    "        print(f\"Expected stoi values to be {expected_i} when sorted, were {i}\")\n",
    "        return\n",
    "    print(\"get_stoi looks good. Onwards!\")\n",
    "test_get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c038c6cd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_itos(stoi):\n",
    "    itos = {stoi[c]:c for c in stoi}\n",
    "    return itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9453e0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def test_get_itos():\n",
    "    stoi = {elem:idx for idx, elem in enumerate(string.ascii_lowercase + \".\")}\n",
    "    itos = get_itos(stoi)\n",
    "    if not isinstance(itos, dict):\n",
    "        print(f\"Expected stoi to be a dict\")\n",
    "        return\n",
    "    for c in string.ascii_lowercase + \".\":\n",
    "        c_i = stoi[c]\n",
    "        if (expected_c := itos[c_i]) != c:\n",
    "            print(f\"Expected itos[{c_i}] to be {expected_c}, was {c}\")\n",
    "    print(\"get_itos looks good. Onwards!\")\n",
    "test_get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c218ec",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "def get_x_and_y(bigrams, stoi):\n",
    "    x = torch.tensor(list(map(lambda bigram : stoi[bigram[0]], bigrams)))\n",
    "    y = torch.tensor(list(map(lambda bigram : stoi[bigram[-1]], bigrams)))\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4895508d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_get_x_and_y():\n",
    "    bigrams = [\n",
    "        ('.', 'h'),\n",
    "        ('h', 'i'),\n",
    "        ('i', '.'),\n",
    "        ('.', 'b'),\n",
    "        ('b', 'y'),\n",
    "        ('y', 'e'),\n",
    "        ('e', '.'),\n",
    "    ]\n",
    "    stoi = {\n",
    "        '.': 0,\n",
    "        'h': 1,\n",
    "        'i': 2,\n",
    "        'b': 3,\n",
    "        'y': 4,\n",
    "        'e': 5,\n",
    "    }\n",
    "    x, y = get_x_and_y(bigrams, stoi)\n",
    "    if (x0 := x[0]) != (expected_x0 := 0):\n",
    "        print(f\"Expected x[0] to be {expected_x0}, was {x0}\")\n",
    "        return\n",
    "    if (y0 := y[0]) != (expected_y0 := 1):\n",
    "        print(f\"Expected y[0] to be {expected_y0}, was {y0}\")\n",
    "        return\n",
    "    if (x_sfe := x[-2]) != (expected_x_sfe := 4):\n",
    "        print(f\"Expected x[-2] to be {expected_x_sfe}, was {x_sfe}\")\n",
    "        return\n",
    "    if (y_sfe := y[-2]) != (expected_y_sfe := 5):\n",
    "        print(f\"Expected y[-2] to be {expected_y_sfe}, was {y_sfe}\")\n",
    "    print(\"get_x_and_y looks good. Onwards!\")\n",
    "test_get_x_and_y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5789e69",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "def initialize_w_b(stoi):\n",
    "    stoi_n = len(stoi)\n",
    "    W = torch.rand((stoi_n,stoi_n), dtype=torch.float64, requires_grad=True)\n",
    "    b = torch.zeros((1,stoi_n),dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5d9a48",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_initialize_w_b():\n",
    "    stoi = {'q': 0, 'w': 1, 'e': 2, 'r': 3}\n",
    "    expected_s_ct = 4\n",
    "    W, b = initialize_w_b(stoi)\n",
    "    if (w_len := len(W)) != expected_s_ct:\n",
    "        print(f\"Expected W to have {expected_s_ct} rows, had {w_len}\")\n",
    "        return\n",
    "    for row_idx in range(w_len):\n",
    "        if (row_len := len(W[row_idx])) != expected_s_ct:\n",
    "            print(f\"Expected W[{row_idx}] to have {expected_s_ct} columns, had {row_len}\")\n",
    "            return\n",
    "        for col_idx in range(row_len):\n",
    "            if (val := W[row_idx][col_idx]) == 0.0:\n",
    "                print(f\"Expected W[{row_idx}][{col_idx}] to be non-zero.\")\n",
    "                return\n",
    "    if not W.requires_grad:\n",
    "        print(\"W must be marked with requires_grad so its grad property will be populated by backpropagation for use in gradient descent.\")\n",
    "        return\n",
    "    if not b.requires_grad:\n",
    "        print(\"b must be marked with requires_grad so its grad property will be populated by backpropagation for use in gradient descent.\")\n",
    "        return\n",
    "    if (b_shape := b.shape) != (expected_b_shape := (1, expected_s_ct)):\n",
    "        print(f\"Expected b to have shape {expected_b_shape}, had shape {b_shape}\")\n",
    "        return\n",
    "    print(\"initialize_w_b looks good. Onwards!\")\n",
    "test_initialize_w_b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea69c5f6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "def forward_prop(x, W, b):\n",
    "    one_hot = torch.nn.functional.one_hot(x, len(W)).double()\n",
    "    output = torch.matmul(one_hot, W) + b\n",
    "\n",
    "    softmax = output.exp()\n",
    "    softmax = softmax / softmax.sum(1, keepdim=True)\n",
    "\n",
    "    return softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3199e7cb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_forward_prop():\n",
    "    x = torch.tensor([\n",
    "        1,\n",
    "        0,\n",
    "    ])\n",
    "\n",
    "    W = torch.tensor([\n",
    "        [0.1, 0.9, 0.2, 0.01],\n",
    "        [0.04, 0.2, 1.6, 0.25],\n",
    "        [0.02, 0.03, 0.7, 0.01],\n",
    "    ], dtype=torch.float64)\n",
    "\n",
    "    b = torch.tensor([\n",
    "        0.01, 0.02, 0.03, 0.04\n",
    "    ], dtype=torch.float64)\n",
    "\n",
    "    expected_y_hat = torch.tensor([\n",
    "        [0.1203, 0.1426, 0.5841, 0.1530],\n",
    "        [0.1881, 0.4228, 0.2120, 0.1771],\n",
    "    ], dtype=torch.float64)\n",
    "\n",
    "    y_hat = forward_prop(x, W, b)\n",
    "\n",
    "    expect_close(\"y_hat\", expected_y_hat, y_hat)\n",
    "    print(\"forward_prop looks good. Onwards!\")\n",
    "test_forward_prop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d364aa60",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "def calculate_loss(y_hat, y):\n",
    "    match_probabilities = y_hat[torch.arange(len(y)), y]\n",
    "    neg_log_likelihood = -match_probabilities.log().mean()\n",
    "    return neg_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286e04bc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "def test_calculate_loss():\n",
    "    y = torch.tensor([2], dtype=torch.int64)\n",
    "    y_hat = torch.tensor([\n",
    "        [0.0, 0.0, 1.0, 0.0]\n",
    "    ])\n",
    "    if abs((loss := calculate_loss(y_hat, y))) > 0.00001:\n",
    "        print(f\"Expected loss for first example to be 0.0, was {loss}\")\n",
    "        return\n",
    "\n",
    "    y = torch.tensor([2, 0], dtype=torch.int64)\n",
    "    y_hat = torch.tensor([\n",
    "        [0.09, 0.09, exp(-0.5), 0.09],\n",
    "        [exp(-0.1), 0.01, 0.02, 0.03]\n",
    "    ])\n",
    "    if abs((loss := calculate_loss(y_hat, y)) - (expected_loss := 0.3)) > 0.00001:\n",
    "        print(f\"Expected loss for second example to be {expected_loss}, was {loss}\")\n",
    "        return\n",
    "    print(\"calculate_loss looks good. Onwards!\")\n",
    "test_calculate_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07b43c1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "def descend_gradient(W, b, learning_rate):\n",
    "    W.data -= learning_rate * W.grad\n",
    "    b.data -= learning_rate * b.grad\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a303744",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "def test_descend_gradient():\n",
    "    W = torch.tensor([\n",
    "        [1.0, 2.0,],\n",
    "        [3.0, -4.0],\n",
    "        [-5.0, 6.0],\n",
    "    ])\n",
    "    W.grad = torch.tensor([\n",
    "        [-2.0, 1.0],\n",
    "        [0.0, -2.0],\n",
    "        [4.0, 1.0]\n",
    "    ])\n",
    "    b = torch.tensor([\n",
    "        1.0,\n",
    "        2.0,\n",
    "    ])\n",
    "    b.grad = torch.tensor([\n",
    "        -1.0,\n",
    "        0.5,\n",
    "    ])\n",
    "    new_w, new_b = descend_gradient(W, b, 3.0)\n",
    "    expected_new_w = torch.tensor([\n",
    "        [7.0, -1.0],\n",
    "        [3.0, 2.0],\n",
    "        [-17.0, 3.0]\n",
    "    ])\n",
    "    expect_close(\"new_w\", expected_new_w, new_w)\n",
    "    expected_new_b = torch.tensor([\n",
    "        4.0,\n",
    "        0.5,\n",
    "    ])\n",
    "    if not new_b.equal(expected_new_b):\n",
    "        print(f\"Expected new b for test case to be \\n{expected_new_b}\\n, is \\n{new_b}\")\n",
    "        return\n",
    "    print(\"descend_gradient looks good. Onward!\")\n",
    "test_descend_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca9a159",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_model(x, y, W, b, learning_rate):\n",
    "    y_hat = forward_prop(x,W,b)\n",
    "    loss = calculate_loss(y_hat, y)\n",
    "    W.grad = None\n",
    "    b.grad = None\n",
    "    loss.backward()\n",
    "    W, b = descend_gradient(W, b, learning_rate)\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25025934",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "def test_train_model():\n",
    "    x = torch.tensor([\n",
    "        0,\n",
    "        1,\n",
    "        2,\n",
    "    ])\n",
    "    y = torch.tensor([\n",
    "        1,\n",
    "        2,\n",
    "        0,\n",
    "    ])\n",
    "    W = torch.tensor([\n",
    "        [1.0, 1.0, 1.0],\n",
    "        [1.0, 1.0, 1.0],\n",
    "        [1.0, 1.0, 1.0],\n",
    "    ], dtype=torch.float64, requires_grad=True)\n",
    "    b = torch.tensor([\n",
    "        0.1,\n",
    "        0.2,\n",
    "        0.3,\n",
    "    ], dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "    loss = train_model(x, y, W, b, 2.0)\n",
    "\n",
    "    expected_W = torch.tensor([\n",
    "        [0.7996, 1.4452, 0.7552],\n",
    "        [0.7996, 0.7785, 1.4219],\n",
    "        [1.4663, 0.7785, 0.7552]\n",
    "    ], dtype=torch.float64)\n",
    "    expect_close(\"W\", expected_W, W)\n",
    "\n",
    "    expected_b = torch.tensor([\n",
    "        0.1654,\n",
    "        0.2022,\n",
    "        0.2323\n",
    "    ], dtype=torch.float64)\n",
    "    expect_close(\"b\", expected_b, b)\n",
    "    print(\"train_model looks good. Onward!\")\n",
    "test_train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9488c18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    words = load_words()\n",
    "    bigrams = generate_bigrams(words)\n",
    "    stoi = get_stoi(bigrams)\n",
    "    itos = get_itos(stoi)\n",
    "    x, y = get_x_and_y(bigrams, stoi)\n",
    "    W, b = initialize_w_b(stoi)\n",
    "    for i in range(1, 101, 1):\n",
    "        loss = train_model(x, y, W, b, 10.0)\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Round {i} loss: {loss}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece100b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
