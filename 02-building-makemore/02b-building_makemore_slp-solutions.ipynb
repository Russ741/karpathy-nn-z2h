{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ca2e5d6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "# Worksheet 2b - Single-Layer Perceptron\n",
    "\n",
    "This is the third in a series of companion worksheets for for Andrej Karpathy's [Neural Networks: Zero To Hero](https://karpathy.ai/zero-to-hero.html) videos.\n",
    "\n",
    "It corresponds to roughly the second half of the second video in the series, named \"[The spelled-out intro to language modeling: building makemore](https://www.youtube.com/watch?v=PaCmpygFfXo)\".\n",
    "\n",
    "The rest of the worksheets are listed in the README [here](https://github.com/Russ741/karpathy-nn-z2h/).\n",
    "\n",
    "The overall objective of this worksheet is to write code that generates a word that is similar to a set of example words it is trained on.\n",
    "It does so using a single-layer neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7113d197",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Preamble: Load data\n",
    "\n",
    "Objective: Load a list of words from the the remotely-hosted [names.txt](https://github.com/karpathy/makemore/blob/master/names.txt) file\n",
    "([raw link](https://github.com/karpathy/makemore/raw/master/names.txt)) into a list variable named ```words```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45db16ed",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Solution code\n",
    "import requests\n",
    "\n",
    "def load_words():\n",
    "    words_url = 'https://raw.githubusercontent.com/karpathy/makemore/master/names.txt'\n",
    "    words = requests.get(words_url).text.splitlines()\n",
    "    return words\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efa3cb5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_words():\n",
    "    if not isinstance(loaded_words, list):\n",
    "        print(f\"Expected words to be a list\")\n",
    "        return\n",
    "    if (len_words := len(loaded_words)) != (expected_words := 32033):\n",
    "        print(f\"Expected {expected_words} elements in words, found {len_words} elements\")\n",
    "        return\n",
    "    if (zeroth_word := loaded_words[0]) != (expected_zeroth := \"emma\"):\n",
    "        print(f\"Expected zeroth word in words to be '{expected_zeroth}', was '{zeroth_word}'\")\n",
    "        return\n",
    "    if (final_word := loaded_words[-1]) != (expected_final := \"zzyzx\"):\n",
    "        print(f\"Expected final word in words to be '{expected_final}', was '{final_word}'\")\n",
    "        return\n",
    "    print(\"load_words looks good. Onwards!\")\n",
    "loaded_words = load_words()\n",
    "test_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5828b9ec",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 1: Generate bigrams\n",
    "\n",
    "Objective: Populate the variable ```bigrams``` with a list of bigrams (2-element tuples) of adjacent characters in ```words```.\n",
    "\n",
    "Treat the start and end of each word as the character '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723987a4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Solution code\n",
    "def generate_bigrams(words):\n",
    "    bigrams = []\n",
    "    for word in words:\n",
    "        bigrams.append(('.', word[0]))\n",
    "        for pos in range(len(word) - 1):\n",
    "            bigrams.append((word[pos], word[pos + 1]))\n",
    "        bigrams.append((word[-1], '.'))\n",
    "    return bigrams\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b26e5d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_generate_bigrams():\n",
    "    bigrams = generate_bigrams(loaded_words)\n",
    "    if not isinstance(bigrams, list):\n",
    "        print(f\"Expected bigrams to be a list\")\n",
    "        return\n",
    "    if (start_m_ct := bigrams.count(('.', 'm'))) != (expected_start_m_ct := 2538):\n",
    "        print(f\"Expected {expected_start_m_ct} ('a', 'b') bigrams, found {start_m_ct}\")\n",
    "        return\n",
    "    if (ab_ct := bigrams.count(('a', 'b'))) != (expected_ab_ct := 541):\n",
    "        print(f\"Expected {expected_ab_ct} ('a', 'b') bigrams, found {ab_ct}\")\n",
    "        return\n",
    "    if (s_end_ct := bigrams.count(('s', '.'))) != (expected_s_end_ct := 1169):\n",
    "        print(f\"Expected {expected_s_end_ct} ('s', '.') bigrams, found {s_end_ct}\")\n",
    "        return\n",
    "    print(\"generate_bigrams looks good. Onwards!\")\n",
    "test_generate_bigrams()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de01f08a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 2: Map characters to indices\n",
    "\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* a list of char, char tuples representing all of the bigrams in a word list\n",
    "\n",
    "And returns:\n",
    "* a dict (```stoi```) where\n",
    "  * the key is a character from ```words``` (including '.' for start/end),\n",
    "  * the value is a unique integer, and\n",
    "  * all the values are in the range from 0 to ```len(stoi) - 1``` (no gaps)\n",
    "\n",
    "We'll use these unique integers as an index to represent the characters in a Tensor in later steps\n",
    "\n",
    "Note that for this list of words, the same value of ```stoi``` could be generated without looking at the words at all,\n",
    "but simply by using all the lowercase letters and a period. This approach would be more efficient for this exercise,\n",
    "but will not generalize well conceptually to more complex models in future exercises.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff8fc52",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Solution code\n",
    "def get_stoi(bigrams):\n",
    "    chars = set()\n",
    "    for bigram in bigrams:\n",
    "        chars.add(bigram[0])\n",
    "        chars.add(bigram[1])\n",
    "    stoi = { v:k for (k, v) in enumerate(sorted(chars))}\n",
    "    return stoi\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec719ff",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def test_get_stoi():\n",
    "    bigrams = [\n",
    "        ('.', 'h'),\n",
    "        ('h', 'i'),\n",
    "        ('i', '.'),\n",
    "        ('.', 'b'),\n",
    "        ('b', 'y'),\n",
    "        ('y', 'e'),\n",
    "        ('e', '.'),\n",
    "    ]\n",
    "    expected_s = sorted(['.', 'h', 'i', 'b', 'y', 'e'])\n",
    "    stoi = get_stoi(bigrams)\n",
    "    if not isinstance(stoi, dict):\n",
    "        print(f\"Expected stoi to be a dict\")\n",
    "        return\n",
    "    s = sorted(stoi.keys())\n",
    "    if s != expected_s:\n",
    "        print(f\"Expected stoi keys to be {expected_s} when sorted, were {s}\")\n",
    "        return\n",
    "    expected_i = list(range(len(s)))\n",
    "    i = sorted(stoi.values())\n",
    "    if i != expected_i:\n",
    "        print(f\"Expected stoi values to be {expected_i} when sorted, were {i}\")\n",
    "        return\n",
    "    print(\"get_stoi looks good. Onwards!\")\n",
    "test_get_stoi()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13bd6d6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 3: Map indices to characters\n",
    "\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* a dict (```stoi```) as defined in step 2\n",
    "\n",
    "And returns:\n",
    "* a dict (```itos```) where ```itos``` contains the same key-value pairs as ```stoi``` but with keys and values swapped.\n",
    "\n",
    "E.g. if ```stoi == {'.' : 0, 'b' : 1, 'z', 2}```, then ```itos == {0 : '.', 1 : 'b', 2 : 'z'}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c038c6cd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Solution code\n",
    "def get_itos(stoi):\n",
    "    itos = {stoi[c]:c for c in stoi}\n",
    "    return itos\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9453e0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def test_get_itos():\n",
    "    stoi = {elem:idx for idx, elem in enumerate(string.ascii_lowercase + \".\")}\n",
    "    itos = get_itos(stoi)\n",
    "    if not isinstance(itos, dict):\n",
    "        print(f\"Expected stoi to be a dict\")\n",
    "        return\n",
    "    for c in string.ascii_lowercase + \".\":\n",
    "        c_i = stoi[c]\n",
    "        if (expected_c := itos[c_i]) != c:\n",
    "            print(f\"Expected itos[{c_i}] to be {expected_c}, was {c}\")\n",
    "    print(\"get_itos looks good. Onwards!\")\n",
    "test_get_itos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6558766",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 4: Split bigrams into inputs and outputs\n",
    "\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* a list ```bigrams``` as defined in step 1, and\n",
    "* a dict ```stoi``` as defined in step 2\n",
    "\n",
    "And returns:\n",
    "* a one-dimensional torch.Tensor ```x``` with all of the first characters in the tuples in ```bigrams```\n",
    "* a one-dimensional torch.Tensor ```y``` with all of the second characters in the tuples in ```bigrams```\n",
    "* Note: Both output tensors should be the same length as ```bigrams```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c218ec",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Solution code\n",
    "import torch\n",
    "\n",
    "def get_x_and_y(bigrams, stoi):\n",
    "    x = torch.tensor(list(map(lambda bigram : stoi[bigram[0]], bigrams)))\n",
    "    y = torch.tensor(list(map(lambda bigram : stoi[bigram[-1]], bigrams)))\n",
    "\n",
    "    return x, y\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4895508d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_get_x_and_y():\n",
    "    bigrams = [\n",
    "        ('.', 'h'),\n",
    "        ('h', 'i'),\n",
    "        ('i', '.'),\n",
    "        ('.', 'b'),\n",
    "        ('b', 'y'),\n",
    "        ('y', 'e'),\n",
    "        ('e', '.'),\n",
    "    ]\n",
    "    stoi = {\n",
    "        '.': 0,\n",
    "        'h': 1,\n",
    "        'i': 2,\n",
    "        'b': 3,\n",
    "        'y': 4,\n",
    "        'e': 5,\n",
    "    }\n",
    "    x, y = get_x_and_y(bigrams, stoi)\n",
    "    if (x0 := x[0]) != (expected_x0 := 0):\n",
    "        print(f\"Expected x[0] to be {expected_x0}, was {x0}\")\n",
    "        return\n",
    "    if (y0 := y[0]) != (expected_y0 := 1):\n",
    "        print(f\"Expected y[0] to be {expected_y0}, was {y0}\")\n",
    "        return\n",
    "    if (x_sfe := x[-2]) != (expected_x_sfe := 4):\n",
    "        print(f\"Expected x[-2] to be {expected_x_sfe}, was {x_sfe}\")\n",
    "        return\n",
    "    if (y_sfe := y[-2]) != (expected_y_sfe := 5):\n",
    "        print(f\"Expected y[-2] to be {expected_y_sfe}, was {y_sfe}\")\n",
    "    print(\"get_x_and_y looks good. Onwards!\")\n",
    "test_get_x_and_y()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff36ee9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 5: Provide initial values for the model parameters\n",
    "\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* a dict ```stoi``` as defined in step 2\n",
    "  * the length of ```stoi``` will be referred to as ```stoi_n```\n",
    "\n",
    "And returns:\n",
    "* a pytorch.Tensor ```W``` of shape (```stoi_n```, ```stoi_n```) where each element is randomly generated\n",
    "* a pytorch.Tensor ```b``` of shape (1, ```stoi_n```)\n",
    "  * The elements of ```b``` can be zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5789e69",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Solution code\n",
    "import torch\n",
    "def initialize_w_b(stoi):\n",
    "    stoi_n = len(stoi)\n",
    "    W = torch.rand((stoi_n,stoi_n), dtype=torch.float64, requires_grad=True)\n",
    "    b = torch.zeros((1,stoi_n),dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "    return W, b\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5d9a48",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_initialize_w_b():\n",
    "    stoi = {'q': 0, 'w': 1, 'e': 2, 'r': 3}\n",
    "    expected_s_ct = 4\n",
    "    W, b = initialize_w_b(stoi)\n",
    "    if (w_len := len(W)) != expected_s_ct:\n",
    "        print(f\"Expected W to have {expected_s_ct} rows, had {w_len}\")\n",
    "        return\n",
    "    for row_idx in range(w_len):\n",
    "        if (row_len := len(W[row_idx])) != expected_s_ct:\n",
    "            print(f\"Expected W[{row_idx}] to have {expected_s_ct} columns, had {row_len}\")\n",
    "            return\n",
    "        for col_idx in range(row_len):\n",
    "            if (val := W[row_idx][col_idx]) == 0.0:\n",
    "                print(f\"Expected W[{row_idx}][{col_idx}] to be non-zero.\")\n",
    "                return\n",
    "    if not W.requires_grad:\n",
    "        print(\"W must be marked with requires_grad so its grad property will be populated by backpropagation for use in gradient descent.\")\n",
    "        return\n",
    "    if not b.requires_grad:\n",
    "        print(\"b must be marked with requires_grad so its grad property will be populated by backpropagation for use in gradient descent.\")\n",
    "        return\n",
    "    if (b_shape := b.shape) != (expected_b_shape := (1, expected_s_ct)):\n",
    "        print(f\"Expected b to have shape {expected_b_shape}, had shape {b_shape}\")\n",
    "        return\n",
    "    print(\"initialize_w_b looks good. Onwards!\")\n",
    "test_initialize_w_b()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ab8275",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 6: Forward propagation\n",
    "\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* a pytorch.Tensor ```x``` of training or testing inputs\n",
    "* pytorch.Tensors ```W``` and ```b``` representing the parameters of the model\n",
    "\n",
    "And returns:\n",
    "* a pytorch.Tensor ```y_hat``` of the model's predicted outputs for each input in x\n",
    "  * The predicted outputs for a given sample should sum to 1.0\n",
    "  * The shape of ```y_hat``` should be (```len(x)```, ```len(W)```)\n",
    "    * Note that ```len(W)``` represents the number of different characters in the word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea69c5f6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Solution code\n",
    "def forward_prop(x, W, b):\n",
    "    one_hot = torch.nn.functional.one_hot(x, len(W)).double()\n",
    "    output = torch.matmul(one_hot, W) + b\n",
    "\n",
    "    softmax = output.exp()\n",
    "    softmax = softmax / softmax.sum(1, keepdim=True)\n",
    "\n",
    "    return softmax\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3199e7cb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_forward_prop():\n",
    "    x = torch.tensor([\n",
    "        1,\n",
    "        0,\n",
    "    ])\n",
    "\n",
    "    W = torch.tensor([\n",
    "        [0.1, 0.9, 0.2, 0.01],\n",
    "        [0.04, 0.2, 1.6, 0.25],\n",
    "        [0.02, 0.03, 0.7, 0.01],\n",
    "    ], dtype=torch.float64)\n",
    "\n",
    "    b = torch.tensor([\n",
    "        0.01, 0.02, 0.03, 0.04\n",
    "    ], dtype=torch.float64)\n",
    "\n",
    "    expected_y_hat = torch.tensor([\n",
    "        [0.1203, 0.1426, 0.5841, 0.1530],\n",
    "        [0.1881, 0.4228, 0.2120, 0.1771],\n",
    "    ], dtype=torch.float64)\n",
    "\n",
    "    y_hat = forward_prop(x, W, b)\n",
    "\n",
    "    if not torch.isclose(expected_y_hat, y_hat, rtol = 0.0, atol = 0.0001).all():\n",
    "        print(f\"Expected y_hat for test case to be \\n{expected_y_hat}\\n, was \\n{y_hat}\")\n",
    "        return\n",
    "    print(\"forward_prop looks good. Onwards!\")\n",
    "test_forward_prop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540ed9eb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 7: Loss calculation\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* a pytorch.Tensor ```y_hat``` of predicted outputs for a particular set of inputs\n",
    "* a pytorch.Tensor ```y``` of actual outputs for the same set of inputs\n",
    "\n",
    "And returns:\n",
    "* a floating-point value representing the model's negative log likelihood loss for that set of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d364aa60",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Solution code\n",
    "def calculate_loss(y_hat, y):\n",
    "    match_probabilities = y_hat[torch.arange(len(y)), y]\n",
    "    neg_log_likelihood = -match_probabilities.log().mean()\n",
    "    return neg_log_likelihood\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286e04bc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "def test_calculate_loss():\n",
    "    y = torch.tensor([2], dtype=torch.int64)\n",
    "    y_hat = torch.tensor([\n",
    "        [0.0, 0.0, 1.0, 0.0]\n",
    "    ])\n",
    "    if abs((loss := calculate_loss(y_hat, y))) > 0.00001:\n",
    "        print(f\"Expected loss for first example to be 0.0, was {loss}\")\n",
    "        return\n",
    "\n",
    "    y = torch.tensor([2, 0], dtype=torch.int64)\n",
    "    y_hat = torch.tensor([\n",
    "        [0.09, 0.09, exp(-0.5), 0.09],\n",
    "        [exp(-0.1), 0.01, 0.02, 0.03]\n",
    "    ])\n",
    "    if abs((loss := calculate_loss(y_hat, y)) - (expected_loss := 0.3)) > 0.00001:\n",
    "        print(f\"Expected loss for second example to be {expected_loss}, was {loss}\")\n",
    "        return\n",
    "    print(\"calculate_loss looks good. Onwards!\")\n",
    "test_calculate_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea94813",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 8: Gradient descent\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* pytorch.Tensors ```W``` and ```b``` representing the parameters of the model\n",
    "* a floating-point value ```learning_rate``` representing the overall size of adjustment to make to the parameters\n",
    "\n",
    "And returns:\n",
    "* the updated pytorch.Tensors ```W``` and ```b```\n",
    "  * Note: Updating the parameters in-place is desirable, but for ease of testing, please return them regardless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07b43c1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Solution code\n",
    "def descend_gradient(W, b, learning_rate):\n",
    "    W.data -= learning_rate * W.grad\n",
    "    b.data -= learning_rate * b.grad\n",
    "    return W, b\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a303744",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_descend_gradient():\n",
    "    W = torch.tensor([\n",
    "        [1.0, 2.0,],\n",
    "        [3.0, -4.0],\n",
    "        [-5.0, 6.0],\n",
    "    ])\n",
    "    W.grad = torch.tensor([\n",
    "        [-2.0, 1.0],\n",
    "        [0.0, -2.0],\n",
    "        [4.0, 1.0]\n",
    "    ])\n",
    "    b = torch.tensor([\n",
    "        1.0,\n",
    "        2.0,\n",
    "    ])\n",
    "    b.grad = torch.tensor([\n",
    "        -1.0,\n",
    "        0.5,\n",
    "    ])\n",
    "    new_w, new_b = descend_gradient(W, b, 3.0)\n",
    "    expected_new_w = torch.tensor([\n",
    "        [7.0, -1.0],\n",
    "        [3.0, 2.0],\n",
    "        [-17.0, 3.0]\n",
    "    ])\n",
    "    if not new_w.equal(expected_new_w):\n",
    "        print(f\"Expected new W for test case to be \\n{expected_new_w}\\n, is \\n{new_w}\")\n",
    "        return\n",
    "    expected_new_b = torch.tensor([\n",
    "        4.0,\n",
    "        0.5,\n",
    "    ])\n",
    "    if not new_b.equal(expected_new_b):\n",
    "        print(f\"Expected new b for test case to be \\n{expected_new_b}\\n, is \\n{new_b}\")\n",
    "        return\n",
    "    print(\"descend_gradient looks good. Onward!\")\n",
    "test_descend_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c56fff7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 9: Train model\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* pytorch.Tensors ```x``` and ```y``` as described in Step 4\n",
    "* pytorch.Tensors ```W``` and ```b``` as described in Step 5\n",
    "* a floating-point value ```learning_rate``` representing the overall size of adjustment to make to the parameters\n",
    "\n",
    "Updates the values of W and b to fit the data slightly better\n",
    "\n",
    "And returns:\n",
    "* the loss as defined in Step 6\n",
    "\n",
    "Implementation note: this function should make use of several of the functions you've previously implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca9a159",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Solution code\n",
    "def train_model(x, y, W, b, learning_rate):\n",
    "    y_hat = forward_prop(x,W,b)\n",
    "    loss = calculate_loss(y_hat, y)\n",
    "    W.grad = None\n",
    "    b.grad = None\n",
    "    loss.backward()\n",
    "    W, b = descend_gradient(W, b, learning_rate)\n",
    "    return loss.item()\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25025934",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_train_model():\n",
    "    x = torch.tensor([\n",
    "        0,\n",
    "        1,\n",
    "        2,\n",
    "    ])\n",
    "    y = torch.tensor([\n",
    "        1,\n",
    "        2,\n",
    "        0,\n",
    "    ])\n",
    "    W = torch.tensor([\n",
    "        [1.0, 1.0, 1.0],\n",
    "        [1.0, 1.0, 1.0],\n",
    "        [1.0, 1.0, 1.0],\n",
    "    ], dtype=torch.float64, requires_grad=True)\n",
    "    b = torch.tensor([\n",
    "        0.1,\n",
    "        0.2,\n",
    "        0.3,\n",
    "    ], dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "    loss = train_model(x, y, W, b, 2.0)\n",
    "\n",
    "    expected_W = torch.tensor([\n",
    "        [0.7996, 1.4452, 0.7552],\n",
    "        [0.7996, 0.7785, 1.4219],\n",
    "        [1.4663, 0.7785, 0.7552]\n",
    "    ], dtype=torch.float64)\n",
    "    if not torch.isclose(expected_W, W, rtol = 0.0, atol = 0.0001).all():\n",
    "        print(f\"Expected W for test case to be \\n{expected_W}\\n, was \\n{W}\")\n",
    "        return\n",
    "\n",
    "    expected_b = torch.tensor([\n",
    "        0.1654,\n",
    "        0.2022,\n",
    "        0.2323\n",
    "    ], dtype=torch.float64)\n",
    "    if not torch.isclose(expected_b, b, rtol = 0.0, atol = 0.0001).all():\n",
    "        print(f\"Expected b for test case to be \\n{expected_b}\\n, was \\n{b}\")\n",
    "        return\n",
    "    print(\"train_model looks good. Onward!\")\n",
    "test_train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9215a7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 10: Generate words\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* pytorch.Tensors ```W``` and ```b``` as described in Step 5\n",
    "* a dict ```stoi``` as described in Step 2\n",
    "* a dict ```itos``` as described in Step 3\n",
    "* a torch.Generator to use for pseudorandom selection of elements\n",
    "\n",
    "Repeatedly generates a probability distribution for the next letter to select given the last letter\n",
    "\n",
    "And returns\n",
    "* a string representing a word generated by repeatedly sampling the probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b817f163",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Solution code\n",
    "def generate_word(W, b, stoi, itos, gen):\n",
    "    chr = '.'\n",
    "    word = \"\"\n",
    "    while True:\n",
    "        x = torch.tensor([stoi[chr]])\n",
    "        probability_distribution = forward_prop(x, W, b)\n",
    "        sample = torch.multinomial(probability_distribution, 1, generator=gen).item()\n",
    "        chr = itos[sample]\n",
    "        if chr == '.':\n",
    "            break\n",
    "        word += chr\n",
    "    return word\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7eaad9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_generate_word():\n",
    "    stoi = {\n",
    "        '.': 0,\n",
    "        'o': 1,\n",
    "        'n': 2,\n",
    "        'w': 3,\n",
    "        'a': 4,\n",
    "        'r': 5,\n",
    "        'd': 6,\n",
    "    }\n",
    "    stoi_n = len(stoi)\n",
    "    itos = {v:k for k,v in stoi.items()}\n",
    "\n",
    "    W = torch.zeros((stoi_n, stoi_n), dtype=torch.float64)\n",
    "    b = torch.zeros((1, stoi_n), dtype=torch.float64)\n",
    "    for i in range(stoi_n - 1):\n",
    "        W[i][i+1] = 1.0\n",
    "    W[stoi_n - 1][0] = 1.0\n",
    "\n",
    "    gen = torch.Generator()\n",
    "    gen.manual_seed(2147476727)\n",
    "    if (word := generate_word(W, b, stoi, itos, gen)) != (expected_word := \"onward\"):\n",
    "        print(f\"Expected word for test case to be {expected_word}, was {word}\")\n",
    "        return\n",
    "    print(f\"generate_word looks good. Onward!\")\n",
    "test_generate_word()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc44d49",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Finale: Put it all together\n",
    "\n",
    "Objective: Write (and call) a function that:\n",
    "* generates the bigrams and character maps\n",
    "* repeatedly trains the model until its loss is acceptably small\n",
    "  * For reference, the \"perfect\" loss of the probability table approach is approximately 2.4241\n",
    "* uses the model to generate some made-up names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9488c18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution code\n",
    "def train_model_and_generate_words():\n",
    "    bigrams = generate_bigrams(loaded_words)\n",
    "    stoi = get_stoi(bigrams)\n",
    "    itos = get_itos(stoi)\n",
    "    x, y = get_x_and_y(bigrams, stoi)\n",
    "    W, b = initialize_w_b(stoi)\n",
    "    for i in range(1, 101, 1):\n",
    "        loss = train_model(x, y, W, b, 10.0)\n",
    "    print(f\"Final loss is {loss}\")\n",
    "    gen = torch.Generator()\n",
    "    for i in range(10):\n",
    "        print(generate_word(W, b, stoi, itos, gen))\n",
    "train_model_and_generate_words()\n",
    "# End solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece100b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
