{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0736e532",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "\n",
    "# Worksheet 2a - Probability Table\n",
    "\n",
    "This is the second in a series of companion worksheets for for Andrej Karpathy's [Neural Networks: Zero To Hero](https://karpathy.ai/zero-to-hero.html) videos.\n",
    "\n",
    "It corresponds to roughly the first half of the second video in the series, named \"[The spelled-out intro to language modeling: building makemore](https://www.youtube.com/watch?v=PaCmpygFfXo)\".\n",
    "\n",
    "The first worksheet in the series is provided by Andrej, and can be found [here](https://colab.research.google.com/drive/1FPTx1RXtBfc4MaTkf7viZZD4U2F9gtKN?usp=sharing).\n",
    "\n",
    "The rest of the worksheets are listed in the README [here](https://github.com/Russ741/karpathy-nn-z2h/).\n",
    "\n",
    "The overall objective of this worksheet is to write code that generates a word that is similar to a set of example words it is trained on.\n",
    "\n",
    "Note that this worksheet uses a probability table, *not* neural networks like subsequent neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313cf76b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Preamble: Load data\n",
    "\n",
    "Objective: Load a list of words from the remotely-hosted [names.txt](https://github.com/karpathy/makemore/blob/master/names.txt) file\n",
    "([raw link](https://github.com/karpathy/makemore/raw/master/names.txt)) into a list variable named ```words```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38442571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76077ae",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_words():\n",
    "    if not isinstance(words, list):\n",
    "        print(f\"Expected words to be a list\")\n",
    "        return\n",
    "    if (len_words := len(words)) != (expected_words := 32033):\n",
    "        print(f\"Expected {expected_words} elements in words, found {len_words} elements\")\n",
    "        return\n",
    "    if (zeroth_word := words[0]) != (expected_zeroth := \"emma\"):\n",
    "        print(f\"Expected zeroth word in words to be '{expected_zeroth}', was '{zeroth_word}'\")\n",
    "        return\n",
    "    if (final_word := words[-1]) != (expected_final := \"zzyzx\"):\n",
    "        print(f\"Expected final word in words to be '{expected_final}', was '{final_word}'\")\n",
    "        return\n",
    "    print(\"words looks good. Onwards!\")\n",
    "test_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb49975b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 1: Generate bigrams\n",
    "\n",
    "Objective: Populate the variable ```bigrams``` with a list of bigrams (2-element tuples) of adjacent characters in ```words```.\n",
    "\n",
    "Treat the start and end of each word as the character '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d928635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15537c49",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_bigrams():\n",
    "    if not isinstance(bigrams, list):\n",
    "        print(f\"Expected bigrams to be a list\")\n",
    "        return\n",
    "    if (start_m_ct := bigrams.count(('.', 'm'))) != (expected_start_m_ct := 2538):\n",
    "        print(f\"Expected {expected_start_m_ct} ('a', 'b') bigrams, found {start_m_ct}\")\n",
    "        return\n",
    "    if (ab_ct := bigrams.count(('a', 'b'))) != (expected_ab_ct := 541):\n",
    "        print(f\"Expected {expected_ab_ct} ('a', 'b') bigrams, found {ab_ct}\")\n",
    "        return\n",
    "    if (s_end_ct := bigrams.count(('s', '.'))) != (expected_s_end_ct := 1169):\n",
    "        print(f\"Expected {expected_s_end_ct} ('s', '.') bigrams, found {s_end_ct}\")\n",
    "        return\n",
    "    print(\"bigrams looks good. Onwards!\")\n",
    "test_bigrams()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db8c22c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 2: Map characters to indices\n",
    "\n",
    "Objective: Build a dict ```stoi``` where the key is a character from ```words``` (including '.' for start/end) and the value is a unique integer.\n",
    "\n",
    "(We'll use these unique integers as an index to represent the characters in a Tensor in later steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cb8bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {}\n",
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70742246",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def test_stoi():\n",
    "    if not isinstance(stoi, dict):\n",
    "        print(f\"Expected stoi to be a dict\")\n",
    "        return\n",
    "    for c in string.ascii_lowercase:\n",
    "        if not c in stoi:\n",
    "            print(f\"Expected {c} to be in stoi\")\n",
    "            return\n",
    "    print(\"stoi looks good. Onwards!\")\n",
    "test_stoi()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5962af37",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 3: Map indices to characters\n",
    "\n",
    "Objective: Build a dict ```itos``` that has the same key-value pairs as ```stoi```, but with each pair's key and value swapped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bddc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = {}\n",
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22331c69",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_itos():\n",
    "    if not isinstance(itos, dict):\n",
    "        print(f\"Expected itos to be a dict\")\n",
    "        return\n",
    "    if (len_itos := len(itos)) != (expected_len := len(stoi)):\n",
    "        print(f\"Expected length to be {expected_len}, was {len_itos}\")\n",
    "        return\n",
    "    for k,v in stoi.items():\n",
    "        if v not in itos:\n",
    "            print(f\"Expected {v} to be a key in itos\")\n",
    "            return\n",
    "        if (itos_v := itos[v]) != (expected_itos_v := k):\n",
    "            print(f\"Expected itos[{v}] to be {expected_itos_v}, was {itos_v}\")\n",
    "            return\n",
    "    print(\"itos looks good. Onwards!\")\n",
    "test_itos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68f8890",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 4: Count occurrences of each bigram\n",
    "\n",
    "Objective: Build a torch Tensor ```N``` where:\n",
    "* the row is the index of the first character in the bigram\n",
    "* the column is the index of the second character in the bigram\n",
    "* the value is the number of times that bigram occurs (represented as an integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7302743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905094d9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_N():\n",
    "    if (N_shape := N.shape) != (expected_N_shape := (27, 27)):\n",
    "        print(f\"Expected shape of N to be {expected_N_shape}, was {N_shape}\")\n",
    "        return\n",
    "    if (N_sum := N.sum()) != (expected_N_sum := 228146):\n",
    "        print(f\"Expected the sum of elements in N to be {expected_N_sum}, was {N_sum}\")\n",
    "        return\n",
    "    if (N_start_m := N[stoi['.']][stoi['m']]) != (expected_N_start_m := 2538):\n",
    "        print(f\"Expected N for ('.', 'm') to be {expected_N_start_m}, was {N_start_m}\")\n",
    "        return\n",
    "    if (N_m_end := N[stoi['m']][stoi['.']]) != (expected_N_m_end := 516):\n",
    "        print(f\"Expected N for ('m', '.') to be {expected_N_m_end}, was {N_m_end}\")\n",
    "        return\n",
    "    print(\"N looks good. Onwards!\")\n",
    "test_N()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b3eb85",
   "metadata": {},
   "source": [
    "### Step 5: Build probability distribution of bigrams\n",
    "\n",
    "Objective: Build a torch Tensor ```P``` where:\n",
    "* the row is the index of the first character in a bigram\n",
    "* the column is the index of the second character in a bigram\n",
    "* the value is the probability (as torch.float64) of a bigram in ```bigrams``` ending with the second character if it starts with the first character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3deebba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc55dd52",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_P():\n",
    "    for row_idx in itos:\n",
    "        if abs(1.0 - (row_sum := P[row_idx].sum().item())) > 0.00001:\n",
    "            row_c = itos[row_idx]\n",
    "            print(f\"Expected the sum of row {row_idx} ({row_c}) to be 1.0, was {row_sum}\")\n",
    "            return\n",
    "    print(\"P looks good. Onwards!\")\n",
    "test_P()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4def4a6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step #6: Write a bigram probability calculation function\n",
    "\n",
    "This is slightly different from the steps that the Karpathy video follows, but will make it easier for this worksheet to verify your code.\n",
    "\n",
    "Write a ```bigram_probability``` function that takes the following arguments:\n",
    "* a 2-element tuple of characters\n",
    "\n",
    "And returns:\n",
    "* a floating-point number from 0.0 to 1.0 that represents the probability of the second character following the first character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bcd608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196cf73d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_bigram_probability():\n",
    "    if (prob_start_end := bigram_probability(('.', '.'))) != (expected_start_end := 0.0):\n",
    "        print(f\"Calculated probability of ('.', '.') is {prob_start_end}, expected {expected_start_end}\")\n",
    "        return\n",
    "    if abs((prob_m_a := bigram_probability(('m', 'a'))) - (expected_m_a := 0.3899)) > 0.001:\n",
    "        print(f\"Calculated probability of ('m', 'a') is {prob_m_a}, expected {expected_m_a}\")\n",
    "        return\n",
    "    print(\"bigram_probability looks good. Onwards!\")\n",
    "test_bigram_probability()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf140e3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step #7: Write a negative log likelihood loss function\n",
    "\n",
    "Write a ```calculate_loss``` function that takes the following arguments:\n",
    "* the name of the bigram probability function written in step 6\n",
    "* a list of bigrams (2-element tuples)\n",
    "\n",
    "And returns:\n",
    "* a floating-point number representing the negative log likelihood of all of the bigrams in the list argument\n",
    "    * Note that Karpathy defines this to be the negative of the *mean* of each tuple's log likelihood, not their sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffdc4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4ecbae",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_calculate_loss():\n",
    "    bigrams = [('.', 'a'), ('a', 'b'), ('b', '.')]\n",
    "    if abs((all_ones := calculate_loss(lambda _ : 1.0, bigrams)) - (expected_all_ones := 0.0)) > 0.0001:\n",
    "        print(f\"Using a probability_func that always returns 1.0 resulted in {all_ones}, expected {expected_all_ones}\")\n",
    "        return\n",
    "    # TODO: Handle zero-probability tuples somehow.\n",
    "    # if (all_zeroes := calculate_loss(lambda _ : 0.0, bigrams)) != (expected_all_zeroes := math.inf):\n",
    "    #    print(f\"Using a probability_func that always returns 0.0 resulted in {all_zeroes}, expected {expected_all_zeroes} \")\n",
    "    #    return\n",
    "    if abs((using_bp := calculate_loss(bigram_probability, bigrams)) - (expected_using_bp := 3.0881)) > 0.0001:\n",
    "        print(f\"Using your bigram_probability function resulted in {using_bp}, expected {expected_using_bp}\")\n",
    "        return\n",
    "    print(\"calculate_loss looks good. Onwards!\")\n",
    "test_calculate_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a16c16e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 8: Calculate ```bigram_probability```'s loss for the bigrams in ```words```\n",
    "\n",
    "Use the function from step #7 to calculate the bigram probability function's loss when given all of the bigrams in ```words```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d591978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_for_words = 0.0\n",
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ca3b77",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_loss_for_words():\n",
    "    if abs(loss_for_words - (expected_loss := 2.4540)) > 0.0001:\n",
    "        print(f\"loss_for_words is {loss_for_words}, expected {expected_loss}\")\n",
    "        return\n",
    "    print(\"loss_for_words looks good. Congratulations!\")\n",
    "test_loss_for_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb09351",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 9: Pick characters based on the probabilities\n",
    "\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* a probability distribution like the one you built in step 5\n",
    "* a ```torch.Generator``` to make the selection process deterministic\n",
    "\n",
    "And returns:\n",
    "* a word (string) generated by repeatedly sampling the probability distribution to determine the next character in the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c417dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc703a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generate_word():\n",
    "    generator = torch.Generator()\n",
    "    generator.manual_seed(2147483645)\n",
    "    if (word := generate_word(P, generator)) != (expected_word := \"machina\"):\n",
    "        print(f\"Generated word was {word}, expected {expected_word}\")\n",
    "        return\n",
    "    print(\"generate_word looks good. Onwards!\")\n",
    "test_generate_word()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fc23d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
