{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eb016f2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Exercises for Andrej Karpathy's [Neural Networks: Zero To Hero](https://karpathy.ai/zero-to-hero.html) videos.\n",
    "\n",
    "This notebook is for Part 2: [The spelled-out intro to language modeling: building makemore](https://www.youtube.com/watch?v=PaCmpygFfXo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff27298",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Preamble: Load data\n",
    "\n",
    "Objective: Load a list of words from the [names.txt](https://github.com/karpathy/makemore/blob/master/names.txt) file into a list variable named ```words```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7342847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load names.txt from the makemore GitHub page:\n",
    "import requests\n",
    "\n",
    "words_url = 'https://raw.githubusercontent.com/karpathy/makemore/master/names.txt'\n",
    "words = requests.get(words_url).text.splitlines()\n",
    "\n",
    "# To load names.txt from a local file:\n",
    "# # curl https://raw.githubusercontent.com/karpathy/makemore/master/names.txt > names.txt\n",
    "#\n",
    "# # read() gets the file as one long string with line breaks in it\n",
    "# # splitlines() divides the whole-file string into a list of strings and removes the line breaks\n",
    "# words = open(\"names.txt\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36eb71c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_words():\n",
    "    if not isinstance(words, list):\n",
    "        print(f\"Expected words to be a list\")\n",
    "        return\n",
    "    if (len_words := len(words)) != (expected_words := 32033):\n",
    "        print(f\"Expected {expected_words} elements in words, found {len_words} elements\")\n",
    "        return\n",
    "    if (zeroth_word := words[0]) != (expected_zeroth := \"emma\"):\n",
    "        print(f\"Expected zeroth word in words to be '{expected_zeroth}', was '{zeroth_word}'\")\n",
    "        return\n",
    "    if (final_word := words[-1]) != (expected_final := \"zzyzx\"):\n",
    "        print(f\"Expected final word in words to be '{expected_final}', was '{final_word}'\")\n",
    "        return\n",
    "    print(\"words looks good. Onwards!\")\n",
    "test_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeb1d3b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 1: Generate bigrams\n",
    "\n",
    "Objective: Populate the variable ```bigrams``` with a list of bigrams (2-element tuples) of adjacent characters in ```words```.\n",
    "\n",
    "Treat the start and end of each word as the character '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65451bf5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "bigrams = []\n",
    "for word in words:\n",
    "    bigrams.append(('.', word[0]))\n",
    "    for pos in range(len(word) - 1):\n",
    "        bigrams.append((word[pos], word[pos + 1]))\n",
    "    bigrams.append((word[-1], '.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df32b03",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_bigrams():\n",
    "    if not isinstance(bigrams, list):\n",
    "        print(f\"Expected bigrams to be a list\")\n",
    "        return\n",
    "    if (start_m_ct := bigrams.count(('.', 'm'))) != (expected_start_m_ct := 2538):\n",
    "        print(f\"Expected {expected_start_m_ct} ('a', 'b') bigrams, found {start_m_ct}\")\n",
    "        return\n",
    "    if (ab_ct := bigrams.count(('a', 'b'))) != (expected_ab_ct := 541):\n",
    "        print(f\"Expected {expected_ab_ct} ('a', 'b') bigrams, found {ab_ct}\")\n",
    "        return\n",
    "    if (s_end_ct := bigrams.count(('s', '.'))) != (expected_s_end_ct := 1169):\n",
    "        print(f\"Expected {expected_s_end_ct} ('s', '.') bigrams, found {s_end_ct}\")\n",
    "        return\n",
    "    print(\"bigrams looks good. Onwards!\")\n",
    "test_bigrams()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e707bc",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 2: Map characters to indices\n",
    "\n",
    "Objective: Build a dict ```stoi``` where the key is a character from ```words``` (including '.' for start/end) and the value is a unique integer.\n",
    "\n",
    "(We'll use these unique integers as an index to represent the characters in a Tensor in later steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167274f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {}\n",
    "chars = set()\n",
    "for bigram in bigrams:\n",
    "    chars.add(bigram[0])\n",
    "    chars.add(bigram[1])\n",
    "stoi = { v:k for (k, v) in enumerate(sorted(chars))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a0a35f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def test_stoi():\n",
    "    if not isinstance(stoi, dict):\n",
    "        print(f\"Expected stoi to be a dict\")\n",
    "        return\n",
    "    for c in string.ascii_lowercase:\n",
    "        if not c in stoi:\n",
    "            print(f\"Expected {c} to be in stoi\")\n",
    "            return\n",
    "    print(\"stoi looks good. Onwards!\")\n",
    "test_stoi()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af01aab",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 3: Map indices to characters\n",
    "\n",
    "Objective: Build a dict ```itos``` that has the same key-value pairs as ```stoi```, but with each pair's key and value swapped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e05c587",
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = {}\n",
    "itos = {stoi[c]:c for c in stoi}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313392df",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 4: Count occurrences of each bigram\n",
    "\n",
    "Objective: Build a torch Tensor ```N``` where:\n",
    "* the row is the index of the first character in the bigram\n",
    "* the column is the index of the second character in the bigram\n",
    "* the value is the number of times that bigram occurs (represented as an integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad1186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "N = torch.zeros(len(stoi), len(stoi), dtype=torch.int32)\n",
    "for bigram in bigrams:\n",
    "    i0 = stoi[bigram[0]]\n",
    "    i1 = stoi[bigram[1]]\n",
    "    N[i0][i1] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8760f1",
   "metadata": {},
   "source": [
    "### Step 5: Build probability distribution of bigrams\n",
    "\n",
    "Objective: Build a torch Tensor ```P``` where:\n",
    "* the row is the index of the first character in a bigram\n",
    "* the column is the index of the second character in a bigram\n",
    "* the value is the probability (as torch.float64) of a bigram in ```bigrams``` ending with the second character if it starts with the first character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b199068",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "P = torch.zeros(len(stoi), len(stoi), dtype=torch.float64)\n",
    "# TODO: Fill in P with probability distributions for bigrams.\n",
    "N_sum = N.sum(1, keepdim=True)\n",
    "P = N / N_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c75da1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_P():\n",
    "    for row_idx in itos:\n",
    "        if abs(1.0 - (row_sum := P[row_idx].sum().item())) > 0.00001:\n",
    "            row_c = itos[row_idx]\n",
    "            print(f\"Expected the sum of row {row_idx} ({row_c}) to be 1.0, was {row_sum}\")\n",
    "            return\n",
    "    print(\"P looks good. Onwards!\")\n",
    "test_P()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f032a09",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step #6: Write a bigram probability calculation function\n",
    "\n",
    "This is slightly different from the steps that the Karpathy video follows, but will make it easier for this worksheet to verify your code.\n",
    "\n",
    "Write a ```bigram_probability``` function that takes the following arguments:\n",
    "* a 2-element tuple of characters\n",
    "\n",
    "And returns:\n",
    "* a floating-point number from 0.0 to 1.0 that represents the probability of the second character following the first character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c416e0a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def bigram_probability(bigram):\n",
    "    return P[stoi[bigram[0]]][stoi[bigram[1]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c172c0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_bigram_probability():\n",
    "    if (prob_start_end := bigram_probability(('.', '.'))) != (expected_start_end := 0.0):\n",
    "        print(f\"Calculated probability of ('.', '.') is {prob_start_end}, expected {expected_start_end}\")\n",
    "        return\n",
    "    if abs((prob_m_a := bigram_probability(('m', 'a'))) - (expected_m_a := 0.3899)) > 0.001:\n",
    "        print(f\"Calculated probability of ('m', 'a') is {prob_m_a}, expected {expected_m_a}\")\n",
    "        return\n",
    "    print(\"bigram_probability looks good. Onwards!\")\n",
    "test_bigram_probability()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2ea89c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step #7: Write a negative log likelihood loss function\n",
    "\n",
    "Write a ```calculate_loss``` function that takes the following arguments:\n",
    "* the name of the bigram probability function written in step 6\n",
    "* a list of bigrams (2-element tuples)\n",
    "\n",
    "And returns:\n",
    "* a floating-point number representing the negative log likelihood of all of the bigrams in the list argument\n",
    "    * Note that Karpathy defines this to be the negative of the *mean* of each tuple's log likelihood, not their sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492d1ccf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_loss(probability_func, bigram_list):\n",
    "    probabilities = list(map(probability_func, bigram_list))\n",
    "    log_probabilities = list(map(math.log, probabilities))\n",
    "    negative_log_likelihood = - sum(log_probabilities) / len(log_probabilities)\n",
    "    return negative_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cc4900",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_calculate_loss():\n",
    "    bigrams = [('.', 'a'), ('a', 'b'), ('b', '.')]\n",
    "    if abs((all_ones := calculate_loss(lambda _ : 1.0, bigrams)) - (expected_all_ones := 0.0)) > 0.0001:\n",
    "        print(f\"Using a probability_func that always returns 1.0 resulted in {all_ones}, expected {expected_all_ones}\")\n",
    "        return\n",
    "    # TODO: Handle zero-probability tuples somehow.\n",
    "    # if (all_zeroes := calculate_loss(lambda _ : 0.0, bigrams)) != (expected_all_zeroes := math.inf):\n",
    "    #    print(f\"Using a probability_func that always returns 0.0 resulted in {all_zeroes}, expected {expected_all_zeroes} \")\n",
    "    #    return\n",
    "    if abs((using_bp := calculate_loss(bigram_probability, bigrams)) - (expected_using_bp := 3.0881)) > 0.0001:\n",
    "        print(f\"Using your bigram_probability function resulted in {using_bp}, expected {expected_using_bp}\")\n",
    "        return\n",
    "    print(\"calculate_loss looks good. Onwards!\")\n",
    "test_calculate_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863f34a4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 8: Calculate ```bigram_probability```'s loss for the bigrams in ```words```\n",
    "\n",
    "Use the function from step #7 to calculate the bigram probability function's loss when given all of the bigrams in ```words```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4105a47e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "loss_for_words = calculate_loss(bigram_probability, bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ca087e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_loss_for_words():\n",
    "    if abs(loss_for_words - (expected_loss := 2.4540)) > 0.0001:\n",
    "        print(f\"loss_for_words is {loss_for_words}, expected {expected_loss}\")\n",
    "        return\n",
    "    print(\"loss_for_words looks good. Congratulations!\")\n",
    "test_loss_for_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d59d7d1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 9: Pick characters based on the probabilities\n",
    "\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* a probability distribution like the one you built in step 5\n",
    "* a ```torch.Generator``` to make the selection process deterministic\n",
    "\n",
    "And returns:\n",
    "* a word (string) generated by repeatedly sampling the probability distribution to determine the next character in the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f794820a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word(probabilities, generator):\n",
    "    current_letter_index = stoi['.']\n",
    "    word = \"\"\n",
    "    while True:\n",
    "        current_letter_index = torch.multinomial(probabilities[current_letter_index], 1, generator=generator).item()\n",
    "        current_letter_index\n",
    "        if current_letter_index == stoi['.']:\n",
    "            break\n",
    "        word += itos[current_letter_index]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564e4241",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generate_word():\n",
    "    generator = torch.Generator()\n",
    "    generator.manual_seed(2147483645)\n",
    "    if (word := generate_word(P, generator)) != (expected_word := \"machina\"):\n",
    "        print(f\"Generated word was {word}, expected {expected_word}\")\n",
    "        return\n",
    "    print(\"generate_word looks good. Onwards!\")\n",
    "test_generate_word()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab3fca4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
