{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "357703cb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "# Worksheet 2b - Single-Layer Perceptron\n",
    "\n",
    "This is the third in a series of companion worksheets for for Andrej Karpathy's [Neural Networks: Zero To Hero](https://karpathy.ai/zero-to-hero.html) videos.\n",
    "\n",
    "It corresponds to roughly the second half of the second video in the series, named \"[The spelled-out intro to language modeling: building makemore](https://www.youtube.com/watch?v=PaCmpygFfXo)\".\n",
    "\n",
    "The rest of the worksheets are listed in the README [here](https://github.com/Russ741/karpathy-nn-z2h/).\n",
    "\n",
    "The overall objective of this worksheet is to write code that generates a word that is similar to a set of example words it is trained on.\n",
    "It does so using a single-layer neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd67f70",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Preamble: Load data\n",
    "\n",
    "Objective: Load a list of words from the the remotely-hosted [names.txt](https://github.com/karpathy/makemore/blob/master/names.txt) file\n",
    "([raw link](https://github.com/karpathy/makemore/raw/master/names.txt)) into a list variable named ```words```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d27d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a115e8c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_words():\n",
    "    if not isinstance(loaded_words, list):\n",
    "        print(f\"Expected words to be a list\")\n",
    "        return\n",
    "    if (len_words := len(loaded_words)) != (expected_words := 32033):\n",
    "        print(f\"Expected {expected_words} elements in words, found {len_words} elements\")\n",
    "        return\n",
    "    if (zeroth_word := loaded_words[0]) != (expected_zeroth := \"emma\"):\n",
    "        print(f\"Expected zeroth word in words to be '{expected_zeroth}', was '{zeroth_word}'\")\n",
    "        return\n",
    "    if (final_word := loaded_words[-1]) != (expected_final := \"zzyzx\"):\n",
    "        print(f\"Expected final word in words to be '{expected_final}', was '{final_word}'\")\n",
    "        return\n",
    "    print(\"load_words looks good. Onwards!\")\n",
    "loaded_words = load_words()\n",
    "test_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b336336",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 1: Generate bigrams\n",
    "\n",
    "Objective: Populate the variable ```bigrams``` with a list of bigrams (2-element tuples) of adjacent characters in ```words```.\n",
    "\n",
    "Treat the start and end of each word as the character '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74456712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded04123",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_generate_bigrams():\n",
    "    bigrams = generate_bigrams(loaded_words)\n",
    "    if not isinstance(bigrams, list):\n",
    "        print(f\"Expected bigrams to be a list\")\n",
    "        return\n",
    "    if (start_m_ct := bigrams.count(('.', 'm'))) != (expected_start_m_ct := 2538):\n",
    "        print(f\"Expected {expected_start_m_ct} ('a', 'b') bigrams, found {start_m_ct}\")\n",
    "        return\n",
    "    if (ab_ct := bigrams.count(('a', 'b'))) != (expected_ab_ct := 541):\n",
    "        print(f\"Expected {expected_ab_ct} ('a', 'b') bigrams, found {ab_ct}\")\n",
    "        return\n",
    "    if (s_end_ct := bigrams.count(('s', '.'))) != (expected_s_end_ct := 1169):\n",
    "        print(f\"Expected {expected_s_end_ct} ('s', '.') bigrams, found {s_end_ct}\")\n",
    "        return\n",
    "    print(\"generate_bigrams looks good. Onwards!\")\n",
    "test_generate_bigrams()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fa6640",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 2: Map characters to indices\n",
    "\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* a list of char, char tuples representing all of the bigrams in a word list\n",
    "\n",
    "And returns:\n",
    "* a dict (```stoi```) where\n",
    "  * the key is a character from ```words``` (including '.' for start/end),\n",
    "  * the value is a unique integer, and\n",
    "  * all the values are in the range from 0 to ```len(stoi) - 1``` (no gaps)\n",
    "\n",
    "We'll use these unique integers as an index to represent the characters in a Tensor in later steps\n",
    "\n",
    "Note that for this list of words, the same value of ```stoi``` could be generated without looking at the words at all,\n",
    "but simply by using all the lowercase letters and a period. This approach would be more efficient for this exercise,\n",
    "but will not generalize well conceptually to more complex models in future exercises.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f0b000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457140ca",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def test_get_stoi():\n",
    "    bigrams = [\n",
    "        ('.', 'h'),\n",
    "        ('h', 'i'),\n",
    "        ('i', '.'),\n",
    "        ('.', 'b'),\n",
    "        ('b', 'y'),\n",
    "        ('y', 'e'),\n",
    "        ('e', '.'),\n",
    "    ]\n",
    "    expected_s = sorted(['.', 'h', 'i', 'b', 'y', 'e'])\n",
    "    stoi = get_stoi(bigrams)\n",
    "    if not isinstance(stoi, dict):\n",
    "        print(f\"Expected stoi to be a dict\")\n",
    "        return\n",
    "    s = sorted(stoi.keys())\n",
    "    if s != expected_s:\n",
    "        print(f\"Expected stoi keys to be {expected_s} when sorted, were {s}\")\n",
    "        return\n",
    "    expected_i = list(range(len(s)))\n",
    "    i = sorted(stoi.values())\n",
    "    if i != expected_i:\n",
    "        print(f\"Expected stoi values to be {expected_i} when sorted, were {i}\")\n",
    "        return\n",
    "    print(\"get_stoi looks good. Onwards!\")\n",
    "test_get_stoi()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124bf0ba",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 3: Map indices to characters\n",
    "\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* a dict (```stoi```) as defined in step 2\n",
    "\n",
    "And returns:\n",
    "* a dict (```itos```) where ```itos``` contains the same key-value pairs as ```stoi``` but with keys and values swapped.\n",
    "\n",
    "E.g. if ```stoi == {'.' : 0, 'b' : 1, 'z', 2}```, then ```itos == {0 : '.', 1 : 'b', 2 : 'z'}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ff4980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dff8c8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def test_get_itos():\n",
    "    stoi = {elem:idx for idx, elem in enumerate(string.ascii_lowercase + \".\")}\n",
    "    itos = get_itos(stoi)\n",
    "    if not isinstance(itos, dict):\n",
    "        print(f\"Expected stoi to be a dict\")\n",
    "        return\n",
    "    for c in string.ascii_lowercase + \".\":\n",
    "        c_i = stoi[c]\n",
    "        if (expected_c := itos[c_i]) != c:\n",
    "            print(f\"Expected itos[{c_i}] to be {expected_c}, was {c}\")\n",
    "    print(\"get_itos looks good. Onwards!\")\n",
    "test_get_itos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d225cfd5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 4: Split bigrams into inputs and outputs\n",
    "\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* a list ```bigrams``` as defined in step 1, and\n",
    "* a dict ```stoi``` as defined in step 2\n",
    "\n",
    "And returns:\n",
    "* a one-dimensional torch.Tensor ```x``` with all of the first characters in the tuples in ```bigrams```\n",
    "* a one-dimensional torch.Tensor ```y``` with all of the second characters in the tuples in ```bigrams```\n",
    "* Note: Both output tensors should be the same length as ```bigrams```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa65044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eedd4b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_get_x_and_y():\n",
    "    bigrams = [\n",
    "        ('.', 'h'),\n",
    "        ('h', 'i'),\n",
    "        ('i', '.'),\n",
    "        ('.', 'b'),\n",
    "        ('b', 'y'),\n",
    "        ('y', 'e'),\n",
    "        ('e', '.'),\n",
    "    ]\n",
    "    stoi = {\n",
    "        '.': 0,\n",
    "        'h': 1,\n",
    "        'i': 2,\n",
    "        'b': 3,\n",
    "        'y': 4,\n",
    "        'e': 5,\n",
    "    }\n",
    "    x, y = get_x_and_y(bigrams, stoi)\n",
    "    if (x0 := x[0]) != (expected_x0 := 0):\n",
    "        print(f\"Expected x[0] to be {expected_x0}, was {x0}\")\n",
    "        return\n",
    "    if (y0 := y[0]) != (expected_y0 := 1):\n",
    "        print(f\"Expected y[0] to be {expected_y0}, was {y0}\")\n",
    "        return\n",
    "    if (x_sfe := x[-2]) != (expected_x_sfe := 4):\n",
    "        print(f\"Expected x[-2] to be {expected_x_sfe}, was {x_sfe}\")\n",
    "        return\n",
    "    if (y_sfe := y[-2]) != (expected_y_sfe := 5):\n",
    "        print(f\"Expected y[-2] to be {expected_y_sfe}, was {y_sfe}\")\n",
    "    print(\"get_x_and_y looks good. Onwards!\")\n",
    "test_get_x_and_y()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa68ac9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 5: Provide initial values for the model parameters\n",
    "\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* a dict ```stoi``` as defined in step 2\n",
    "  * the length of ```stoi``` will be referred to as ```stoi_n```\n",
    "\n",
    "And returns:\n",
    "* a pytorch.Tensor ```W``` of shape (```stoi_n```, ```stoi_n```) where each element is randomly generated\n",
    "* a pytorch.Tensor ```b``` of shape (1, ```stoi_n```)\n",
    "  * The elements of ```b``` can be zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da9ecd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5317f0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_initialize_w_b():\n",
    "    stoi = {'q': 0, 'w': 1, 'e': 2, 'r': 3}\n",
    "    expected_s_ct = 4\n",
    "    W, b = initialize_w_b(stoi)\n",
    "    if (w_len := len(W)) != expected_s_ct:\n",
    "        print(f\"Expected W to have {expected_s_ct} rows, had {w_len}\")\n",
    "        return\n",
    "    for row_idx in range(w_len):\n",
    "        if (row_len := len(W[row_idx])) != expected_s_ct:\n",
    "            print(f\"Expected W[{row_idx}] to have {expected_s_ct} columns, had {row_len}\")\n",
    "            return\n",
    "        for col_idx in range(row_len):\n",
    "            if (val := W[row_idx][col_idx]) == 0.0:\n",
    "                print(f\"Expected W[{row_idx}][{col_idx}] to be non-zero.\")\n",
    "                return\n",
    "    if not W.requires_grad:\n",
    "        print(\"W must be marked with requires_grad so its grad property will be populated by backpropagation for use in gradient descent.\")\n",
    "        return\n",
    "    if not b.requires_grad:\n",
    "        print(\"b must be marked with requires_grad so its grad property will be populated by backpropagation for use in gradient descent.\")\n",
    "        return\n",
    "    if (b_shape := b.shape) != (expected_b_shape := (1, expected_s_ct)):\n",
    "        print(f\"Expected b to have shape {expected_b_shape}, had shape {b_shape}\")\n",
    "        return\n",
    "    print(\"initialize_w_b looks good. Onwards!\")\n",
    "test_initialize_w_b()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1d079f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 6: Forward propagation\n",
    "\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* a pytorch.Tensor ```x``` of training or testing inputs\n",
    "* pytorch.Tensors ```W``` and ```b``` representing the parameters of the model\n",
    "\n",
    "And returns:\n",
    "* a pytorch.Tensor ```y_hat``` of the model's predicted outputs for each input in x\n",
    "  * The predicted outputs for a given sample should sum to 1.0\n",
    "  * The shape of ```y_hat``` should be (```len(x)```, ```len(W)```)\n",
    "    * Note that ```len(W)``` represents the number of different characters in the word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686d9594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d372f0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_forward_prop():\n",
    "    x = torch.tensor([\n",
    "        1,\n",
    "        0,\n",
    "    ])\n",
    "\n",
    "    W = torch.tensor([\n",
    "        [0.1, 0.9, 0.2, 0.01],\n",
    "        [0.04, 0.2, 1.6, 0.25],\n",
    "        [0.02, 0.03, 0.7, 0.01],\n",
    "    ], dtype=torch.float64)\n",
    "\n",
    "    b = torch.tensor([\n",
    "        0.01, 0.02, 0.03, 0.04\n",
    "    ], dtype=torch.float64)\n",
    "\n",
    "    expected_y_hat = torch.tensor([\n",
    "        [0.1203, 0.1426, 0.5841, 0.1530],\n",
    "        [0.1881, 0.4228, 0.2120, 0.1771],\n",
    "    ], dtype=torch.float64)\n",
    "\n",
    "    y_hat = forward_prop(x, W, b)\n",
    "\n",
    "    if not torch.isclose(expected_y_hat, y_hat, rtol = 0.0, atol = 0.0001).all():\n",
    "        print(f\"Expected y_hat for test case to be \\n{expected_y_hat}\\n, was \\n{y_hat}\")\n",
    "        return\n",
    "    print(\"forward_prop looks good. Onwards!\")\n",
    "test_forward_prop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e186fb1b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 7: Loss calculation\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* a pytorch.Tensor ```y_hat``` of predicted outputs for a particular set of inputs\n",
    "* a pytorch.Tensor ```y``` of actual outputs for the same set of inputs\n",
    "\n",
    "And returns:\n",
    "* a floating-point value representing the model's negative log likelihood loss for that set of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febe33b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cf56b0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "def test_calculate_loss():\n",
    "    y = torch.tensor([2], dtype=torch.int64)\n",
    "    y_hat = torch.tensor([\n",
    "        [0.0, 0.0, 1.0, 0.0]\n",
    "    ])\n",
    "    if abs((loss := calculate_loss(y_hat, y))) > 0.00001:\n",
    "        print(f\"Expected loss for first example to be 0.0, was {loss}\")\n",
    "        return\n",
    "\n",
    "    y = torch.tensor([2, 0], dtype=torch.int64)\n",
    "    y_hat = torch.tensor([\n",
    "        [0.09, 0.09, exp(-0.5), 0.09],\n",
    "        [exp(-0.1), 0.01, 0.02, 0.03]\n",
    "    ])\n",
    "    if abs((loss := calculate_loss(y_hat, y)) - (expected_loss := 0.3)) > 0.00001:\n",
    "        print(f\"Expected loss for second example to be {expected_loss}, was {loss}\")\n",
    "        return\n",
    "    print(\"calculate_loss looks good. Onwards!\")\n",
    "test_calculate_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621444d5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 8: Gradient descent\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* pytorch.Tensors ```W``` and ```b``` representing the parameters of the model\n",
    "* a floating-point value ```learning_rate``` representing the overall size of adjustment to make to the parameters\n",
    "\n",
    "And returns:\n",
    "* the updated pytorch.Tensors ```W``` and ```b```\n",
    "  * Note: Updating the parameters in-place is desirable, but for ease of testing, please return them regardless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f71727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c63eed7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_descend_gradient():\n",
    "    W = torch.tensor([\n",
    "        [1.0, 2.0,],\n",
    "        [3.0, -4.0],\n",
    "        [-5.0, 6.0],\n",
    "    ])\n",
    "    W.grad = torch.tensor([\n",
    "        [-2.0, 1.0],\n",
    "        [0.0, -2.0],\n",
    "        [4.0, 1.0]\n",
    "    ])\n",
    "    b = torch.tensor([\n",
    "        1.0,\n",
    "        2.0,\n",
    "    ])\n",
    "    b.grad = torch.tensor([\n",
    "        -1.0,\n",
    "        0.5,\n",
    "    ])\n",
    "    new_w, new_b = descend_gradient(W, b, 3.0)\n",
    "    expected_new_w = torch.tensor([\n",
    "        [7.0, -1.0],\n",
    "        [3.0, 2.0],\n",
    "        [-17.0, 3.0]\n",
    "    ])\n",
    "    if not new_w.equal(expected_new_w):\n",
    "        print(f\"Expected new W for test case to be \\n{expected_new_w}\\n, is \\n{new_w}\")\n",
    "        return\n",
    "    expected_new_b = torch.tensor([\n",
    "        4.0,\n",
    "        0.5,\n",
    "    ])\n",
    "    if not new_b.equal(expected_new_b):\n",
    "        print(f\"Expected new b for test case to be \\n{expected_new_b}\\n, is \\n{new_b}\")\n",
    "        return\n",
    "    print(\"descend_gradient looks good. Onward!\")\n",
    "test_descend_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5509668",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 9: Train model\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* pytorch.Tensors ```x``` and ```y``` as described in Step 4\n",
    "* pytorch.Tensors ```W``` and ```b``` as described in Step 5\n",
    "* a floating-point value ```learning_rate``` representing the overall size of adjustment to make to the parameters\n",
    "\n",
    "Updates the values of W and b to fit the data slightly better\n",
    "\n",
    "And returns:\n",
    "* the loss as defined in Step 6\n",
    "\n",
    "Implementation note: this function should make use of several of the functions you've previously implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d698d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d51e246",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_train_model():\n",
    "    x = torch.tensor([\n",
    "        0,\n",
    "        1,\n",
    "        2,\n",
    "    ])\n",
    "    y = torch.tensor([\n",
    "        1,\n",
    "        2,\n",
    "        0,\n",
    "    ])\n",
    "    W = torch.tensor([\n",
    "        [1.0, 1.0, 1.0],\n",
    "        [1.0, 1.0, 1.0],\n",
    "        [1.0, 1.0, 1.0],\n",
    "    ], dtype=torch.float64, requires_grad=True)\n",
    "    b = torch.tensor([\n",
    "        0.1,\n",
    "        0.2,\n",
    "        0.3,\n",
    "    ], dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "    loss = train_model(x, y, W, b, 2.0)\n",
    "\n",
    "    expected_W = torch.tensor([\n",
    "        [0.7996, 1.4452, 0.7552],\n",
    "        [0.7996, 0.7785, 1.4219],\n",
    "        [1.4663, 0.7785, 0.7552]\n",
    "    ], dtype=torch.float64)\n",
    "    if not torch.isclose(expected_W, W, rtol = 0.0, atol = 0.0001).all():\n",
    "        print(f\"Expected W for test case to be \\n{expected_W}\\n, was \\n{W}\")\n",
    "        return\n",
    "\n",
    "    expected_b = torch.tensor([\n",
    "        0.1654,\n",
    "        0.2022,\n",
    "        0.2323\n",
    "    ], dtype=torch.float64)\n",
    "    if not torch.isclose(expected_b, b, rtol = 0.0, atol = 0.0001).all():\n",
    "        print(f\"Expected b for test case to be \\n{expected_b}\\n, was \\n{b}\")\n",
    "        return\n",
    "    print(\"train_model looks good. Onward!\")\n",
    "test_train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf582849",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Step 10: Generate words\n",
    "Objective: Write a function that takes the following arguments:\n",
    "* pytorch.Tensors ```W``` and ```b``` as described in Step 5\n",
    "* a dict ```stoi``` as described in Step 2\n",
    "* a dict ```itos``` as described in Step 3\n",
    "* a torch.Generator to use for pseudorandom selection of elements\n",
    "\n",
    "Repeatedly generates a probability distribution for the next letter to select given the last letter\n",
    "\n",
    "And returns\n",
    "* a string representing a word generated by repeatedly sampling the probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d75463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3425c382",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def test_generate_word():\n",
    "    stoi = {\n",
    "        '.': 0,\n",
    "        'o': 1,\n",
    "        'n': 2,\n",
    "        'w': 3,\n",
    "        'a': 4,\n",
    "        'r': 5,\n",
    "        'd': 6,\n",
    "    }\n",
    "    stoi_n = len(stoi)\n",
    "    itos = {v:k for k,v in stoi.items()}\n",
    "\n",
    "    W = torch.zeros((stoi_n, stoi_n), dtype=torch.float64)\n",
    "    b = torch.zeros((1, stoi_n), dtype=torch.float64)\n",
    "    for i in range(stoi_n - 1):\n",
    "        W[i][i+1] = 1.0\n",
    "    W[stoi_n - 1][0] = 1.0\n",
    "\n",
    "    gen = torch.Generator()\n",
    "    gen.manual_seed(2147476727)\n",
    "    if (word := generate_word(W, b, stoi, itos, gen)) != (expected_word := \"onward\"):\n",
    "        print(f\"Expected word for test case to be {expected_word}, was {word}\")\n",
    "        return\n",
    "    print(f\"generate_word looks good. Onward!\")\n",
    "test_generate_word()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c3cbeb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Finale: Put it all together\n",
    "\n",
    "Objective: Write (and call) a function that:\n",
    "* generates the bigrams and character maps\n",
    "* repeatedly trains the model until its loss is acceptably small\n",
    "  * For reference, the \"perfect\" loss of the probability table approach is approximately 2.4241\n",
    "* uses the model to generate some made-up names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfecc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c085814f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
